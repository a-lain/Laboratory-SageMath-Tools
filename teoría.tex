\documentclass[11pt,a4paper,spanish]{article}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=3cm]{geometry}
\usepackage[utf8x]{inputenc}
\usepackage[]{graphicx}
\usepackage{babel}
\usepackage{varioref}
\usepackage{amssymb,amsmath,amsthm}
\usepackage{amsfonts}
\usepackage{makeidx}
\usepackage{graphicx}
\usepackage{float}
\usepackage{pdflscape}
\usepackage{bm}
\usepackage{multirow}
\usepackage{footnote}
\usepackage{color}
\usepackage{hyperref}
\usepackage{subcaption}
\usepackage{fancyhdr}
\usepackage{mathtools}
\usepackage{cases}
\usepackage[breakable]{tcolorbox}
\usepackage[Export]{adjustbox} % Used to constrain images to a maximum size
\usepackage{listings}
\renewcommand{\arraystretch}{1.3}
\newcommand{\dbar}{{\mathchar'26\mkern-12mu \mathrm{d}}}
\newcommand{\der}[2]{{\dfrac{\text{d}#1 }{\text{d}#2 }}}
\newcommand{\sder}[2]{{\dfrac{\text{d}^2 #1 }{\text{d}#2^2}}}
\renewcommand{\qedsymbol}{$\mathbb{Q}.\mathbb{E}.\mathbb{D}.$}


\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
\numberwithin{equation}{section}
\numberwithin{table}{section}
\numberwithin{figure}{section}
\theoremstyle{definition}
\newtheorem{defn}{\protect\definitionname}[section]
\theoremstyle{remark}
\newtheorem{notation}{\protect\notationname}[section]
\theoremstyle{definition}
\newtheorem{example}{\protect\examplename}[section]
\theoremstyle{remark}
\newtheorem{rem}{\protect\remarkname}[section]
\theoremstyle{plain}
\newtheorem{prop}{\protect\propositionname}[section]
\theoremstyle{plain}
\newtheorem{lem}{\protect\lemmaname}[section]
\theoremstyle{plain}
\newtheorem{cor}{\protect\corollaryname}[section]
\theoremstyle{plain}
\newtheorem{thm}{\protect\theoremname}[section]
\theoremstyle{plain}
\newtheorem{assumption}{\protect\assumptionname}[section]
\theoremstyle{plain}
\newtheorem{lyxalgorithm}{\protect\algorithmname}[section]

\providecommand{\exercisename}{Ejercicio}
\providecommand{\solutionname}{Solución}
%\providecommand{\assumptionname}{Postulado}
\providecommand{\algorithmname}{Implementación en el ordenador}
\renewcommand{\arraystretch}{1.5}

\makeatother

\providecommand{\algorithmname}{Algoritmo}
\providecommand{\assumptionname}{Suposición}
\providecommand{\corollaryname}{Corolario}
\providecommand{\definitionname}{Definición}
\providecommand{\examplename}{Ejemplo}
\providecommand{\lemmaname}{Lema}
\providecommand{\notationname}{Notación}
\providecommand{\propositionname}{Proposición}
\providecommand{\remarkname}{Observación}
\providecommand{\theoremname}{Teorema}


% Cosas varias
\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{mygray}{rgb}{0.5,0.5,0.5}
\definecolor{mymauve}{rgb}{0.58,0,0.82}

\lstset{
	backgroundcolor=\color{white},   % choose the background color; you must add \usepackage{color} or \usepackage{xcolor}; should come as last argument
	basicstyle=\footnotesize,        % the size of the fonts that are used for the code
	breakatwhitespace=false,         % sets if automatic breaks should only happen at whitespace
	breaklines=true,                 % sets automatic line breaking
	captionpos=b,                    % sets the caption-position to bottom
	commentstyle=\color{mygreen},    % comment style
	deletekeywords={...},            % if you want to delete keywords from the given language
	escapeinside={\%*}{*)},          % if you want to add LaTeX within your code
	extendedchars=true,              % lets you use non-ASCII characters; for 8-bits encodings only, does not work with UTF-8
	firstnumber=1,                % start line enumeration with line 1000
	frame=single,	                   % adds a frame around the code
	keepspaces=true,                 % keeps spaces in text, useful for keeping indentation of code (possibly needs columns=flexible)
	keywordstyle=\color{blue},		 % keyword style     
	language=Python,                 % the language of the code
	morekeywords={*,...},            % if you want to add more keywords to the set
	numbers=left,                    % where to put the line-numbers; possible values are (none, left, right)
	numbersep=5pt,                   % how far the line-numbers are from the code
	numberstyle=\tiny\color{mygray}, % the style that is used for the line-numbers
	rulecolor=\color{black},         % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. comments (green here))
	showspaces=false,                % show spaces everywhere adding particular underscores; it overrides 'showstringspaces'
	showstringspaces=false,          % underline spaces within strings only
	showtabs=false,                  % show tabs within strings adding particular underscores
	stepnumber=2,                    % the step between two line-numbers. If it's 1, each line will be numbered
	stringstyle=\color{mymauve},     % string literal style
	tabsize=2,	                   % sets default tabsize to 2 spaces
	title=\lstname,                % show the filename of files included with \lstinputlisting; also try caption instead of title
	texcl=false,
	literate=%
	{±}{{$\pm$}}1
	{σ}{{$\sigma$}}1
	{á}{{\'a}}1
	{í}{{\'i}}1
	{é}{{\'e}}1
	{ý}{{\'y}}1
	{ú}{{\'u}}1
	{ó}{{\'o}}1
	{Á}{{\'A}}1
	{É}{{\'E}}1
	{Í}{{\'I}}1
	{Ó}{{\'O}}1
	{Ú}{{\'U}}1
	{ñ}{{\~n}}1
	{Ñ}{{\~N}}1
	{ü}{{\"u}}1
	{Ü}{{\"U}}1
	{¿}{{?'}}1
	{¡}{{!'}}1
	{·}{{$\cdot$}}1,
}




\usepackage[breakable]{tcolorbox}
\usepackage{parskip} % Stop auto-indenting (to mimic markdown behaviour)



% Basic figure setup, for now with no caption control since it's done
% automatically by Pandoc (which extracts ![](path) syntax from Markdown).
% Maintain compatibility with old templates. Remove in nbconvert 6.0
\let\Oldincludegraphics\includegraphics
% Ensure that by default, figures have no caption (until we provide a
% proper Figure object with a Caption API and a way to capture that
% in the conversion process - todo).


\usepackage[Export]{adjustbox} % Used to constrain images to a maximum size
\adjustboxset{max size={1\linewidth}{1\paperheight}}
\floatplacement{figure}{H} % forces figures to be placed at the correct location
\usepackage{enumerate} % Needed for markdown enumerations to work
\usepackage{textcomp} % defines textquotesingle
% Hack from http://tex.stackexchange.com/a/47451/13684:
\AtBeginDocument{%
	\def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
}
\usepackage{upquote} % Upright quotes for verbatim code
\usepackage{eurosym} % defines \euro
\usepackage{fancyvrb} % verbatim replacement that allows latex
\usepackage{grffile} % extends the file name processing of package graphics 
% to support a larger range
\makeatletter % fix for grffile with XeLaTeX
\def\Gread@@xetex#1{%
	\IfFileExists{"\Gin@base".bb}%
	{\Gread@eps{\Gin@base.bb}}%
	{\Gread@@xetex@aux#1}%
}
\makeatother

% The hyperref package gives us a pdf with properly built
% internal navigation ('pdf bookmarks' for the table of contents,
% internal cross-reference links, web links for URLs, etc.)
% The default LaTeX title has an obnoxious amount of whitespace. By default,
% titling removes some of it. It also provides customization options.
\usepackage{titling}
\usepackage{longtable} % longtable support required by pandoc >1.10
\usepackage{booktabs}  % table support for pandoc > 1.12.2
\usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
\usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
% normalem makes italics be italics, not underlines
\usepackage{mathrsfs}



% Colors for the hyperref package
\definecolor{urlcolor}{rgb}{0,.145,.698}
\definecolor{linkcolor}{rgb}{.71,0.21,0.01}
\definecolor{citecolor}{rgb}{.12,.54,.11}

% ANSI colors
\definecolor{ansi-black}{HTML}{3E424D}
\definecolor{ansi-black-intense}{HTML}{282C36}
\definecolor{ansi-red}{HTML}{E75C58}
\definecolor{ansi-red-intense}{HTML}{B22B31}
\definecolor{ansi-green}{HTML}{00A250}
\definecolor{ansi-green-intense}{HTML}{007427}
\definecolor{ansi-yellow}{HTML}{DDB62B}
\definecolor{ansi-yellow-intense}{HTML}{B27D12}
\definecolor{ansi-blue}{HTML}{208FFB}
\definecolor{ansi-blue-intense}{HTML}{0065CA}
\definecolor{ansi-magenta}{HTML}{D160C4}
\definecolor{ansi-magenta-intense}{HTML}{A03196}
\definecolor{ansi-cyan}{HTML}{60C6C8}
\definecolor{ansi-cyan-intense}{HTML}{258F8F}
\definecolor{ansi-white}{HTML}{C5C1B4}
\definecolor{ansi-white-intense}{HTML}{A1A6B2}
\definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
\definecolor{ansi-default-inverse-bg}{HTML}{000000}

% commands and environments needed by pandoc snippets
% extracted from the output of `pandoc -s`
\providecommand{\tightlist}{%
	\setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\newenvironment{Shaded}{}{}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
\newcommand{\RegionMarkerTok}[1]{{#1}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
\newcommand{\NormalTok}[1]{{#1}}

% Additional commands for more recent versions of Pandoc
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
\newcommand{\ImportTok}[1]{{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
\newcommand{\BuiltInTok}[1]{{#1}}
\newcommand{\ExtensionTok}[1]{{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}


% Define a nice break command that doesn't care if a line doesn't already
% exist.
\def\br{\hspace*{\fill} \\* }
% Math Jax compatibility definitions
\def\gt{>}
\def\lt{<}
\let\Oldtex\TeX
\let\Oldlatex\LaTeX
\renewcommand{\TeX}{\textrm{\Oldtex}}
\renewcommand{\LaTeX}{\textrm{\Oldlatex}}
% Document parameters
% Document title





% Pygments definitions
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
	\let\PY@ul=\relax \let\PY@tc=\relax%
	\let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
	\PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
				\PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


% For linebreaks inside Verbatim environment from package fancyvrb. 
\makeatletter
\newbox\Wrappedcontinuationbox 
\newbox\Wrappedvisiblespacebox 
\newcommand*\Wrappedvisiblespace {\textcolor{red}{\textvisiblespace}} 
\newcommand*\Wrappedcontinuationsymbol {\textcolor{red}{\llap{\tiny$\m@th\hookrightarrow$}}} 
\newcommand*\Wrappedcontinuationindent {3ex } 
\newcommand*\Wrappedafterbreak {\kern\Wrappedcontinuationindent\copy\Wrappedcontinuationbox} 
% Take advantage of the already applied Pygments mark-up to insert 
% potential linebreaks for TeX processing. 
%        {, <, #, %, $, ' and ": go to next line. 
	%        _, }, ^, &, >, - and ~: stay at end of broken line. 
% Use of \textquotesingle for straight quote. 
\newcommand*\Wrappedbreaksatspecials {% 
	\def\PYGZus{\discretionary{\char`\_}{\Wrappedafterbreak}{\char`\_}}% 
	\def\PYGZob{\discretionary{}{\Wrappedafterbreak\char`\{}{\char`\{}}% 
	\def\PYGZcb{\discretionary{\char`\}}{\Wrappedafterbreak}{\char`\}}}% 
	\def\PYGZca{\discretionary{\char`\^}{\Wrappedafterbreak}{\char`\^}}% 
	\def\PYGZam{\discretionary{\char`\&}{\Wrappedafterbreak}{\char`\&}}% 
	\def\PYGZlt{\discretionary{}{\Wrappedafterbreak\char`\<}{\char`\<}}% 
	\def\PYGZgt{\discretionary{\char`\>}{\Wrappedafterbreak}{\char`\>}}% 
	\def\PYGZsh{\discretionary{}{\Wrappedafterbreak\char`\#}{\char`\#}}% 
	\def\PYGZpc{\discretionary{}{\Wrappedafterbreak\char`\%}{\char`\%}}% 
	\def\PYGZdl{\discretionary{}{\Wrappedafterbreak\char`\$}{\char`\$}}% 
	\def\PYGZhy{\discretionary{\char`\-}{\Wrappedafterbreak}{\char`\-}}% 
	\def\PYGZsq{\discretionary{}{\Wrappedafterbreak\textquotesingle}{\textquotesingle}}% 
	\def\PYGZdq{\discretionary{}{\Wrappedafterbreak\char`\"}{\char`\"}}% 
	\def\PYGZti{\discretionary{\char`\~}{\Wrappedafterbreak}{\char`\~}}% 
} 
% Some characters . , ; ? ! / are not pygmentized. 
% This macro makes them "active" and they will insert potential linebreaks 
\newcommand*\Wrappedbreaksatpunct {% 
	\lccode`\~`\.\lowercase{\def~}{\discretionary{\hbox{\char`\.}}{\Wrappedafterbreak}{\hbox{\char`\.}}}% 
	\lccode`\~`\,\lowercase{\def~}{\discretionary{\hbox{\char`\,}}{\Wrappedafterbreak}{\hbox{\char`\,}}}% 
	\lccode`\~`\;\lowercase{\def~}{\discretionary{\hbox{\char`\;}}{\Wrappedafterbreak}{\hbox{\char`\;}}}% 
	\lccode`\~`\:\lowercase{\def~}{\discretionary{\hbox{\char`\:}}{\Wrappedafterbreak}{\hbox{\char`\:}}}% 
	\lccode`\~`\?\lowercase{\def~}{\discretionary{\hbox{\char`\?}}{\Wrappedafterbreak}{\hbox{\char`\?}}}% 
	\lccode`\~`\!\lowercase{\def~}{\discretionary{\hbox{\char`\!}}{\Wrappedafterbreak}{\hbox{\char`\!}}}% 
	\lccode`\~`\/\lowercase{\def~}{\discretionary{\hbox{\char`\/}}{\Wrappedafterbreak}{\hbox{\char`\/}}}% 
	\catcode`\.\active
	\catcode`\,\active 
	\catcode`\;\active
	\catcode`\:\active
	\catcode`\?\active
	\catcode`\!\active
	\catcode`\/\active 
	\lccode`\~`\~ 	
}
\makeatother

\let\OriginalVerbatim=\Verbatim
\makeatletter
\renewcommand{\Verbatim}[1][1]{%
	%\parskip\z@skip
	\sbox\Wrappedcontinuationbox {\Wrappedcontinuationsymbol}%
	\sbox\Wrappedvisiblespacebox {\FV@SetupFont\Wrappedvisiblespace}%
	\def\FancyVerbFormatLine ##1{\hsize\linewidth
		\vtop{\raggedright\hyphenpenalty\z@\exhyphenpenalty\z@
			\doublehyphendemerits\z@\finalhyphendemerits\z@
			\strut ##1\strut}%
	}%
	% If the linebreak is at a space, the latter will be displayed as visible
	% space at end of first line, and a continuation symbol starts next line.
	% Stretch/shrink are however usually zero for typewriter font.
	\def\FV@Space {%
		\nobreak\hskip\z@ plus\fontdimen3\font minus\fontdimen4\font
		\discretionary{\copy\Wrappedvisiblespacebox}{\Wrappedafterbreak}
		{\kern\fontdimen2\font}%
	}%
	
	% Allow breaks at special characters using \PYG... macros.
	\Wrappedbreaksatspecials
	% Breaks at punctuation characters . , ; ? ! and / need catcode=\active 	
	\OriginalVerbatim[#1,codes*=\Wrappedbreaksatpunct]%
}
\makeatother

% Exact colors from NB
\definecolor{incolor}{HTML}{303F9F}
\definecolor{outcolor}{HTML}{D84315}
\definecolor{cellborder}{HTML}{CFCFCF}
\definecolor{cellbackground}{HTML}{F7F7F7}

% prompt
\makeatletter
\newcommand{\boxspacing}{\kern\kvtcb@left@rule\kern\kvtcb@boxsep}
\makeatother
\newcommand{\prompt}[4]{
	\ttfamily\llap{{\color{#2}[#3]:\hspace{3pt}#4}}\vspace{-\baselineskip}
}






\title{Teoría detrás de las herramientas de laboratorio}
\author{Miguel Calvo Arnal, Juan Guerrero Marcos y Andrés Laín Sanclemente}
\date{}
\begin{document}
	\maketitle
	\newpage
	\tableofcontents
	\newpage
	
	\begin{notation}
		Para las medidas no usamos ningún tipo de símbolo en especial $x$,
		para la variable aleatoria asociada a una medida $x$ usaremos la
		notación $\mathring{x}$, para la variable asociada a la variable
		aleatoria $\mathring{x}$ y que aparece como argumento en su función
		de densidad de probabilidad y de distribución usaremos $\dot{x}$.
		Para un valor ideal o <<real>> de una medida $x$ usaremos $\tilde{x}$
		y, por último, para denotar un estimador de una variable aleatoria
		$\mathring{x}$ usaremos $\hat{x}$.
	\end{notation}
	\section{Distribución normal o gaussiana}
	\begin{prop}
		\label{prop:gaussiana + constante}Sea $\mathring{x}$ una variable
		aleatoria gaussiana de centro $\mu$ y varianza $\sigma^{2}$. Entonces
		$\mathring{y}=\mathring{x}+b$, donde $b\in\mathbb{R}$ es una constante,
		es también una variable aleatoria gaussiana de varianza $\sigma^{2}$,
		pero de centro $\mu+b$.
		\[
		\mathring{x}\sim\mathcal{N}\left(\mu,\sigma^{2}\right)\implies\mathring{y}=\mathring{x}+b\sim\mathcal{N}\left(\mu+b,\sigma^{2}\right)\qquad\forall b\in\mathbb{R}
		\]
	\end{prop}
	\begin{proof}
		La función de distribución de la variable aleatoria $\mathring{y}$
		se relaciona con la de $\mathring{x}$ mediante:
		\[
		F_{\mathring{y}}\left(\dot{y}\right)=\mathrm{P}\left(\mathring{y}<\dot{y}\right)=\mathrm{P}\left(\mathring{x}+b<\dot{y}\right)=\mathrm{P}\left(\mathring{x}<\dot{y}-b\right)=F_{\mathring{x}}\left(\dot{y}-b\right)
		\]
		Derivando la expresión anterior, obtenemos:
		\[
		f_{\mathring{y}}\left(\dot{y}\right)=\frac{\mathrm{d}F_{\mathring{y}}}{\mathrm{d}\dot{y}}\left(\dot{y}\right)=\frac{\mathrm{d}F_{\mathring{x}}}{\mathrm{d}\dot{x}}\left(\dot{y}-b\right)=f_{\mathring{x}}\left(\dot{y}-b\right)=
		\]
		\[
		=\frac{1}{\sqrt{2\pi\sigma^{2}}}\exp\left(-\frac{\left(\dot{y}-b-\mu\right)^{2}}{2\sigma^{2}}\right)=\frac{1}{\sqrt{2\pi\sigma^{2}}}\exp\left(-\frac{\left(\dot{y}-\left(\mu+b\right)\right)^{2}}{2\sigma^{2}}\right)
		\]
		que claramente se corresponde con la la función de densidad de una
		variable aleatoria gaussiana de centro $\mu+b$ y varianza $\sigma^{2}$.
	\end{proof}
	\begin{prop}
		\label{prop:gaussiana * constante}Sea $\mathring{x}$ una variable
		aleatoria gaussiana de centro $\mu$ y varianza $\sigma^{2}$. Entonces
		$\mathring{y}=a\mathring{x}$, donde $a\in\mathbb{R}$ es una constante,
		es también una variable aleatoria normal, pero de centro $a\mu$ y
		de varianza $a^{2}\sigma^{2}$.
		
		\[
		\mathring{x}\sim\mathcal{N}\left(\mu,\sigma^{2}\right)\implies\mathring{y}=a\mathring{x}\sim\mathcal{N}\left(a\mu,a^{2}\sigma^{2}\right)\qquad\forall a\in\mathbb{R}
		\]
	\end{prop}
	\begin{proof}
		La función de densidad de la variable aleatoria $\mathring{y}$ se
		relaciona con la de $\mathring{x}$ mediante:
		\[
		F_{\mathring{y}}\left(\dot{y}\right)=\mathrm{P}\left(\mathring{y}<\dot{y}\right)=\mathrm{P}\left(a\mathring{x}<\dot{y}\right)=\mathrm{P}\left(\mathring{x}<\frac{\dot{y}}{a}\right)=F_{\mathring{x}}\left(\frac{\dot{y}}{a}\right)
		\]
		Derivando la expresión anterior, obtenemos:
		\[
		f_{\mathring{y}}\left(\dot{y}\right)=\frac{\mathrm{d}F_{\mathring{y}}}{\mathrm{d}\dot{y}}\left(\dot{y}\right)=\frac{\mathrm{d}F_{\mathring{x}}}{\mathrm{d}\dot{x}}\left(\frac{\dot{y}}{a}\right)\frac{1}{a}=\frac{1}{a}f_{\mathring{x}}\left(\frac{\dot{y}}{a}\right)=
		\]
		\[
		=\frac{1}{a}\frac{1}{\sqrt{2\pi\sigma^{2}}}\exp\left(-\frac{\left(\frac{\dot{y}}{a}-\mu\right)^{2}}{2\sigma^{2}}\right)=\frac{1}{\sqrt{2\pi a^{2}\sigma^{2}}}\exp\left(-\frac{\left(\frac{\dot{y}-a\mu}{a}\right)^{2}}{2\sigma^{2}}\right)=
		\]
		\[
		=\frac{1}{\sqrt{2\pi a^{2}\sigma^{2}}}\exp\left(-\frac{\left(\dot{y}-a\mu\right)^{2}}{2a^{2}\sigma^{2}}\right)
		\]
		Y lo anterior es claramente la función de densidad de probabilidad
		de una variable aleatoria gaussiana de centro $a\mu$ y varianza $a^{2}\sigma^{2}$.
	\end{proof}
	\begin{prop}
		\label{prop:gaussiana normalizable}Toda variable aleatoria normal
		puede <<normalizarse>> a una variable aleatoria gaussiana de centro
		$0$ y varianza $1$.
		\[
		\mathring{x}\sim\mathcal{N}\left(\mu,\sigma^{2}\right)\implies\mathring{z}=\frac{\mathring{x}-\mu}{\sigma}\sim\mathcal{N}\left(0,1\right)
		\]
	\end{prop}
	\begin{proof}
		Sea $\mathring{x}\sim\mathcal{N}\left(\mu,\sigma^{2}\right)$. Por
		la proposición \vref{prop:gaussiana + constante}, la variable aleatoria
		$\mathring{y}=\mathring{x}-\mu$, cumple $\mathring{y}\sim\mathcal{N}\left(0,\sigma^{2}\right)$.
		Por último, por la proposición \vref{prop:gaussiana * constante},
		la variable aleatoria $\mathring{z}=\frac{1}{\sigma}\mathring{y}$
		satisface $\mathring{z}\sim\mathcal{N}\left(0,1\right)$. Por tanto:
		\[
		\mathring{z}=\frac{\mathring{y}}{\sigma}=\frac{\mathring{x}-\mu}{\sigma}\sim\mathcal{N}\left(0,1\right)
		\]
	\end{proof}
	\begin{prop}
		\label{prop:Gauss suma}La combinación lineal de dos variables aleatorias
		gaussianas normalizadas es otra variable aleatoria normal de centro
		$0$ y varianza $a^{2}+b^{2}$, siendo $a$ y $b$ los coeficientes
		de la combinación lineal.
		\[
		\mathring{x}_{1},\mathring{x}_{2}\sim\mathcal{N}\left(0,1\right)\implies\mathring{y}=a\mathring{x}_{1}+b\mathring{x}_{2}\sim\mathcal{N}\left(0,a^{2}+b^{2}\right)
		\]
		con $a,b\in\mathbb{R}$.
	\end{prop}
	\begin{proof}
		Intentemos obtener la función de distribución de la variable $\mathring{y}$:
		\[
		F_{\mathring{y}}\left(\dot{y}\right)=\mathrm{P}\left(\mathring{y}<\dot{y}\right)=\mathrm{P}\left(a\mathring{x}_{1}+b\mathring{x}_{2}<\dot{y}\right)=\iint_{\left\{ a\dot{x}_{1}+b\dot{x}_{2}<\dot{y}\right\} }f_{\mathring{x}_{1}}\left(\dot{x}_{1}\right)f_{\mathring{x}_{2}}\left(\dot{x}_{2}\right)\mathrm{d}\dot{x}_{1}\mathrm{d}\dot{x}_{2}=
		\]
		\[
		=\iint_{\left\{ a\dot{x}_{1}+b\dot{x}_{2}<\dot{y}\right\} }\frac{1}{\sqrt{2\pi}}\exp\left(-\frac{\dot{x}_{1}^{2}}{2}\right)\frac{1}{\sqrt{2\pi}}\exp\left(-\frac{\dot{x}_{2}^{2}}{2}\right)\mathrm{d}\dot{x}_{1}\mathrm{d}\dot{x}_{2}=
		\]
		\[
		=\iint_{\left\{ a\dot{x}_{1}+b\dot{x}_{2}<\dot{y}\right\} }\frac{1}{2\pi}\exp\left(-\frac{\dot{x}_{1}^{2}}{2}-\frac{\dot{x}_{2}^{2}}{2}\right)\mathrm{d}\dot{x}_{1}\mathrm{d}\dot{x}_{2}=
		\]
		\[
		=\iint_{\left\{ a\dot{x}_{1}+b\dot{x}_{2}<\dot{y}\right\} }\frac{1}{2\pi}\exp\left(-\frac{\dot{x}_{1}^{2}+\dot{x}_{2}^{2}}{2}\right)\mathrm{d}\dot{x}_{1}\mathrm{d}\dot{x}_{2}
		\]
		A continuación, hacemos el cambio de variable:
		\[
		\dot{u}_{1}=a\dot{x}_{1}+b\dot{x}_{2}
		\]
		\[
		\dot{u}_{2}=a\dot{x}_{1}-b\dot{x}_{2}
		\]
		Sumando las dos ecuaciones anteriores, podemos despejar $\dot{x}_{1}$:
		\[
		\dot{u}_{1}+\dot{u}_{2}=2a\dot{x}_{1}\iff\dot{x}_{1}=\frac{\dot{u}_{1}+\dot{u}_{2}}{2a}
		\]
		Y restando las dos ecuaciones anteriores, podemos despejar $\dot{x}_{2}$:
		\[
		\dot{u}_{1}-\dot{u}_{2}=2b\dot{x}_{2}\iff\dot{x}_{2}=\frac{\dot{u}_{1}-\dot{u}_{2}}{2b}
		\]
		Calculamos ahora la matriz jacobiana del cambio $\left(\dot{u}_{1},\dot{u}_{2}\right)\longrightarrow\left(\dot{x}_{1},\dot{x}_{2}\right)$:
		\[
		\mathrm{J}=\left(\begin{matrix}\frac{\partial\dot{x}_{1}}{\partial\dot{u}_{1}} & \frac{\partial\dot{x}_{1}}{\partial\dot{u}_{2}}\\
			\frac{\partial\dot{x}_{2}}{\partial\dot{u}_{1}} & \frac{\partial\dot{x}_{2}}{\partial\dot{u}_{2}}
		\end{matrix}\right)=\left(\begin{matrix}\frac{1}{2a} & \frac{1}{2a}\\
			\frac{1}{2b} & -\frac{1}{2b}
		\end{matrix}\right)
		\]
		\[
		\left|\det J\right|=\left|-\frac{1}{4ab}-\frac{1}{4ab}\right|=\frac{1}{2ab}
		\]
		Por otra parte, el conjunto de integración queda:
		\[
		\left\{ \left(\dot{x}_{1},\dot{x}_{2}\right)\in\mathbb{R}^{2}\text{ t.q. }a\dot{x}_{1}+b\dot{x}_{2}<\dot{y}\right\} =\left\{ \left(\dot{u}_{1},\dot{u}_{2}\right)\in(-\infty,\dot{y})\times\mathbb{R}\right\} 
		\]
		Con este cambio de variable el dominio de integración ha quedado infinitamente
		más sencillo de manejar. De esta forma, por el teorema del cambio
		de variable, la integral resulta:
		\[
		F_{\mathring{y}}\left(\dot{y}\right)=\int_{\dot{u}_{1}=-\infty}^{\dot{y}}\int_{\dot{u}_{2}=-\infty}^{\infty}\frac{1}{2ab}\frac{1}{2\pi}\exp\left(-\frac{\left(\frac{\dot{u}_{1}+\dot{u}_{2}}{2a}\right)^{2}+\left(\frac{\dot{u}_{1}-\dot{u}_{2}}{2b}\right)^{2}}{2}\right)\mathrm{d}\dot{u}_{1}\mathrm{d}\dot{u}_{2}=
		\]
		\[
		=\int_{\dot{u}_{1}=-\infty}^{\dot{y}}\int_{\dot{u}_{2}=-\infty}^{\infty}\frac{1}{4\pi ab}\exp\left(-\frac{\frac{\dot{u}_{1}^{2}+\dot{u}_{2}^{2}+2\dot{u}_{1}\dot{u}_{2}}{4a^{2}}+\frac{\dot{u}_{1}^{2}+\dot{u}_{2}^{2}-2\dot{u}_{1}\dot{u}_{2}}{4b^{2}}}{2}\right)\mathrm{d}\dot{u}_{1}\mathrm{d}\dot{u}_{2}=
		\]
		\[
		=\int_{\dot{u}_{1}=-\infty}^{\dot{y}}\int_{\dot{u}_{2}=-\infty}^{\infty}\frac{1}{4\pi ab}\exp\left(-\frac{\frac{\dot{u}_{1}^{2}+\dot{u}_{2}^{2}+2\dot{u}_{1}\dot{u}_{2}}{a^{2}}+\frac{\dot{u}_{1}^{2}+\dot{u}_{2}^{2}-2\dot{u}_{1}\dot{u}_{2}}{b^{2}}}{8}\right)\mathrm{d}\dot{u}_{1}\mathrm{d}\dot{u}_{2}=
		\]
		\[
		=\int_{\dot{u}_{1}=-\infty}^{\dot{y}}\int_{\dot{u}_{2}=-\infty}^{\infty}\frac{1}{4\pi ab}\exp\left[-\frac{b^{2}\left(\dot{u}_{1}^{2}+\dot{u}_{2}^{2}+2\dot{u}_{1}\dot{u}_{2}\right)+a^{2}\left(\dot{u}_{1}^{2}+\dot{u}_{2}^{2}-2\dot{u}_{1}\dot{u}_{2}\right)}{8a^{2}b^{2}}\right]\mathrm{d}\dot{u}_{1}\mathrm{d}\dot{u}_{2}=
		\]
		\begin{equation}
			=\int_{\dot{u}_{1}=-\infty}^{\dot{y}}\int_{\dot{u}_{2}=-\infty}^{\infty}\frac{1}{4\pi ab}\exp\left[-\frac{\left(b^{2}+a^{2}\right)\dot{u}_{1}^{2}+\left(b^{2}+a^{2}\right)\dot{u}_{2}^{2}+2\dot{u}_{1}\dot{u}_{2}\left(b^{2}-a^{2}\right)}{8a^{2}b^{2}}\right]\mathrm{d}\dot{u}_{1}\mathrm{d}\dot{u}_{2}\label{eq:Suma Gauss int}
		\end{equation}
		A continuación, nuestro propósito es completar cuadrados con el fin
		de poder hacer la integral en la variable $\dot{u}_{2}$. Para ello,
		notemos:
		\[
		\left(\sqrt{b^{2}+a^{2}}\dot{u}_{2}+\dot{u}_{1}\frac{\left(b^{2}-a^{2}\right)}{\sqrt{b^{2}+a^{2}}}\right)^{2}=
		\]
		\[
		=\left(b^{2}+a^{2}\right)\dot{u}_{2}^{2}+\dot{u}_{1}^{2}\frac{\left(b^{2}-a^{2}\right)^{2}}{b^{2}+a^{2}}+2\dot{u}_{1}\dot{u}_{2}\left(b^{2}-a^{2}\right)\iff
		\]
		\[
		\iff\left(\sqrt{b^{2}+a^{2}}\dot{u}_{2}+\dot{u}_{1}\frac{\left(b^{2}-a^{2}\right)}{\sqrt{b^{2}+a^{2}}}\right)^{2}-\dot{u}_{1}^{2}\frac{\left(b^{2}-a^{2}\right)^{2}}{b^{2}+a^{2}}=\left(b^{2}+a^{2}\right)\dot{u}_{2}^{2}+2\dot{u}_{1}\dot{u}_{2}\left(b^{2}-a^{2}\right)
		\]
		De esta forma, podemos expresar el exponente del integrando de la
		expresión \vref{eq:Suma Gauss int} como:
		\[
		-\frac{\left(\sqrt{b^{2}+a^{2}}\dot{u}_{2}+\dot{u}_{1}\frac{\left(b^{2}-a^{2}\right)}{\sqrt{b^{2}+a^{2}}}\right)^{2}-\dot{u}_{1}^{2}\frac{\left(b^{2}-a^{2}\right)^{2}}{b^{2}+a^{2}}+\left(b^{2}+a^{2}\right)\dot{u}_{1}^{2}}{8a^{2}b^{2}}=
		\]
		\[
		=-\frac{\left(\sqrt{b^{2}+a^{2}}\dot{u}_{2}+\dot{u}_{1}\frac{\left(b^{2}-a^{2}\right)}{\sqrt{b^{2}+a^{2}}}\right)^{2}+\frac{\left(b^{2}+a^{2}\right)^{2}-\left(b^{2}-a^{2}\right)^{2}}{b^{2}+a^{2}}\dot{u}_{1}^{2}}{8a^{2}b^{2}}=
		\]
		\[
		=-\frac{\left(\sqrt{b^{2}+a^{2}}\dot{u}_{2}+\dot{u}_{1}\frac{\left(b^{2}-a^{2}\right)}{\sqrt{b^{2}+a^{2}}}\right)^{2}+\frac{b^{4}+a^{4}+2a^{2}b^{2}-b^{4}-a^{4}+2a^{2}b^{2}}{b^{2}+a^{2}}\dot{u}_{1}^{2}}{8a^{2}b^{2}}=
		\]
		\[
		=-\frac{\left(\sqrt{b^{2}+a^{2}}\dot{u}_{2}+\dot{u}_{1}\frac{\left(b^{2}-a^{2}\right)}{\sqrt{b^{2}+a^{2}}}\right)^{2}+\frac{4a^{2}b^{2}}{b^{2}+a^{2}}\dot{u}_{1}^{2}}{8a^{2}b^{2}}=
		\]
		\[
		=-\frac{1}{2}\left(\frac{\sqrt{b^{2}+a^{2}}\dot{u}_{2}+\dot{u}_{1}\frac{\left(b^{2}-a^{2}\right)}{\sqrt{b^{2}+a^{2}}}}{2ab}\right)^{2}-\frac{\dot{u}_{1}^{2}}{2\left(a^{2}+b^{2}\right)}
		\]
		Sustituyendo de vuelta en la integral de la expresión \vref{eq:Suma Gauss int},
		se tiene:
		\[
		F_{\mathring{y}}\left(\dot{y}\right)=\int_{\dot{u}_{1}=-\infty}^{\dot{y}}\int_{\dot{u}_{2}=-\infty}^{\infty}\frac{1}{4\pi ab}\exp\left[-\frac{1}{2}\left(\frac{\sqrt{b^{2}+a^{2}}\dot{u}_{2}+\dot{u}_{1}\frac{\left(b^{2}-a^{2}\right)}{\sqrt{b^{2}+a^{2}}}}{2ab}\right)^{2}-\frac{\dot{u}_{1}^{2}}{2\left(a^{2}+b^{2}\right)}\right]\mathrm{d}\dot{u}_{1}\mathrm{d}\dot{u}_{2}=
		\]
		\[
		=\int_{\dot{u}_{1}=-\infty}^{\dot{y}}\int_{\dot{u}_{2}=-\infty}^{\infty}\frac{1}{4\pi ab}\exp\left[-\frac{1}{2}\left(\frac{\sqrt{b^{2}+a^{2}}\dot{u}_{2}+\dot{u}_{1}\frac{\left(b^{2}-a^{2}\right)}{\sqrt{b^{2}+a^{2}}}}{2ab}\right)^{2}\right]\exp\left[-\frac{\dot{u}_{1}^{2}}{2\left(a^{2}+b^{2}\right)}\right]\mathrm{d}\dot{u}_{1}\mathrm{d}\dot{u}_{2}
		\]
		Por el teorema de Fubini, se tiene:
		\begin{equation}
			F_{\mathring{y}}\left(\dot{y}\right)=\int_{\dot{u}_{1}=-\infty}^{\dot{y}}\frac{1}{4\pi ab}\exp\left[-\frac{\dot{u}_{1}^{2}}{2\left(a^{2}+b^{2}\right)}\right]\underbrace{\left(\int_{\dot{u}_{2}=-\infty}^{\infty}\exp\left[-\frac{1}{2}\left(\frac{\sqrt{b^{2}+a^{2}}\dot{u}_{2}+\dot{u}_{1}\frac{\left(b^{2}-a^{2}\right)}{\sqrt{b^{2}+a^{2}}}}{2ab}\right)^{2}\right]\mathrm{d}\dot{u}_{2}\right)}_{\eqqcolon I}\mathrm{d}\dot{u}_{1}\label{eq:Gauss suma int 2}
		\end{equation}
		Para proseguir, solucionaremos la integral $I$ de forma separada.
		Para ello, recuérdese que dentro de esta integral $\dot{u}_{1}$ no
		es más que una constante. De esta forma, hacemos el cambio de variable:
		\[
		\dot{t}\coloneqq\frac{\sqrt{b^{2}+a^{2}}\dot{u}_{2}+\dot{u}_{1}\frac{\left(b^{2}-a^{2}\right)}{\sqrt{b^{2}+a^{2}}}}{2ab}\iff2ab\dot{t}=\sqrt{b^{2}+a^{2}}\dot{u}_{2}+\dot{u}_{1}\frac{\left(b^{2}-a^{2}\right)}{\sqrt{b^{2}+a^{2}}}\iff
		\]
		\[
		\iff2ab\dot{t}-\dot{u}_{1}\frac{\left(b^{2}-a^{2}\right)}{\sqrt{b^{2}+a^{2}}}=\sqrt{b^{2}+a^{2}}\dot{u}_{2}\iff
		\]
		\[
		\iff\dot{u}_{2}=\frac{2ab\dot{t}-\dot{u}_{1}\frac{\left(b^{2}-a^{2}\right)}{\sqrt{b^{2}+a^{2}}}}{\sqrt{a^{2}+b^{2}}}
		\]
		Derivando, se tiene:
		\[
		\mathrm{d}\dot{u}_{2}=\frac{2ab}{\sqrt{a^{2}+b^{2}}}\mathrm{d}\dot{t}
		\]
		Además:
		\[
		\dot{u}_{2}\to\infty\iff\dot{t}\to\infty
		\]
		\[
		\dot{u}_{2}\to-\infty\iff\dot{t}\to-\infty
		\]
		Por consiguiente, la integral $I$ queda:
		\[
		I=\int_{\dot{t}=-\infty}^{\infty}\exp\left(-\frac{1}{2}\dot{t}^{2}\right)\frac{2ab}{\sqrt{a^{2}+b^{2}}}\mathrm{d}\dot{t}=
		\]
		\[
		=\frac{2ab}{\sqrt{a^{2}+b^{2}}}\,\sqrt{2\pi}\int_{\dot{t}=-\infty}^{\infty}\underbrace{\frac{1}{\sqrt{2\pi}}\exp\left(-\frac{1}{2}\dot{t}^{2}\right)}_{*}\mathrm{d}\dot{t}
		\]
		donde el término marcado es claramente la función de densidad de una
		variable aleatoria gaussiana de centro $0$ y varianza unidad: $\mathring{t}\sim\mathcal{N}\left(0,1\right)$.
		Por tanto, la integral que nos resta por evaluar debe ser la unidad
		por la definición de función de densidad. Así, se llega a:
		\[
		I=\frac{2ab}{\sqrt{a^{2}+b^{2}}}\,\sqrt{2\pi}
		\]
		Sustituyendo de vuelta en la integral \vref{eq:Gauss suma int 2},
		se tiene:
		\[
		F_{\mathring{y}}\left(\dot{y}\right)=\int_{\dot{u}_{1}=-\infty}^{\dot{y}}\frac{1}{4\pi ab}\exp\left(-\frac{\dot{u}_{1}^{2}}{2\left(a^{2}+b^{2}\right)}\right)\frac{2ab}{\sqrt{a^{2}+b^{2}}}\,\sqrt{2\pi}\mathrm{d}\dot{u}_{1}=
		\]
		\[
		=\int_{\dot{u}_{1}=-\infty}^{\dot{y}}\frac{\sqrt{2\pi}}{2\pi}\frac{1}{\sqrt{a^{2}+b^{2}}}\exp\left(-\frac{\dot{u}_{1}^{2}}{2\left(a^{2}+b^{2}\right)}\right)\mathrm{d}\dot{u}_{1}=
		\]
		\[
		=\int_{\dot{u}_{1}=-\infty}^{\dot{y}}\frac{1}{\sqrt{2\pi\left(a^{2}+b^{2}\right)}}\exp\left(-\frac{\dot{u}_{1}^{2}}{2\left(a^{2}+b^{2}\right)}\right)\mathrm{d}\dot{u}_{1}
		\]
		A continuación, por el Teorema Fundamental del Cálculo Integral, se
		tiene:
		\[
		f_{\mathring{y}}\left(\dot{y}\right)=\frac{\mathrm{d}F_{\mathring{y}}}{\mathrm{d}\dot{y}}\left(\dot{y}\right)=\frac{1}{\sqrt{2\pi\left(a^{2}+b^{2}\right)}}\exp\left(-\frac{\dot{y}^{2}}{2\left(a^{2}+b^{2}\right)}\right)
		\]
		que es claramente la función de densidad de una variable aleatoria
		normal de centro $0$ y de varianza $a^{2}+b^{2}$. Por ende:
		\[
		\mathring{y}\sim\mathcal{N}\left(0,a^{2}+b^{2}\right)
		\]
	\end{proof}
	\begin{cor}
		\label{cor:Suma gauss 2}La combinación lineal de dos variables aleatorias
		gaussianas es otra variable aleatoria gaussiana. En concreto:
		\[
		\left.\begin{matrix}\mathring{x}_{1}\sim\mathcal{N}\left(\mu_{1},\sigma_{1}^{2}\right)\\
			\mathring{x}_{1}\sim\mathcal{N}\left(\mu_{2},\sigma_{2}^{2}\right)
		\end{matrix}\right\} \implies\mathring{y}=a\mathring{x}_{1}+b\mathring{x}_{2}\sim\mathcal{N}\left(a\mu_{1}+b\mu_{2},a^{2}\sigma_{1}^{2}+b^{2}\sigma_{2}^{2}\right)
		\]
		con $a,b\in\mathbb{R}$.
	\end{cor}
	\begin{proof}
		Primero, nótese que:
		\[
		\mathring{y}=a\mathring{x}_{1}+b\mathring{x}_{2}=a\left(\mathring{x}_{1}-\mu_{1}+\mu_{1}\right)+b\left(\mathring{x}_{2}-\mu_{2}+\mu_{2}\right)=
		\]
		\[
		=a\left(\mathring{x}_{1}-\mu_{1}\right)+a\mu_{1}+b\left(\mathring{x}_{2}-\mu_{2}\right)+b\mu_{2}=
		\]
		\[
		=a\sigma_{1}\left(\frac{\mathring{x}_{1}-\mu_{1}}{\sigma_{1}}\right)+a\mu_{1}+b\sigma_{2}\left(\frac{\mathring{x}_{2}-\mu_{2}}{\sigma_{2}}\right)+b\mu_{2}=
		\]
		\begin{equation}
			=a\mu_{1}+b\mu_{2}+a\sigma_{1}\left(\frac{\mathring{x}_{1}-\mu_{1}}{\sigma_{1}}\right)+b\sigma_{2}\left(\frac{\mathring{x}_{2}-\mu_{2}}{\sigma_{2}}\right)\label{eq:Suma gauss y}
		\end{equation}
		Ahora bien, por la proposición \vref{prop:gaussiana normalizable}:
		\[
		\mathring{z}_{1}\coloneqq\frac{\mathring{x}_{1}-\mu_{1}}{\sigma_{1}}\qquad\mathring{z}_{2}\coloneqq\frac{\mathring{x}_{2}-\mu_{2}}{\sigma_{2}}
		\]
		son ambas variables aleatorias normales estándar: $\mathring{z}_{1},\mathring{z}_{2}\sim\mathcal{N}\left(0,1\right)$.
		Por tanto, el término;
		\[
		a\sigma_{1}\left(\frac{\mathring{x}_{1}-\mu_{1}}{\sigma_{1}}\right)+b\sigma_{2}\left(\frac{\mathring{x}_{2}-\mu_{2}}{\sigma_{2}}\right)=a\sigma_{1}\mathring{z}_{1}+b\sigma_{2}\mathring{z}_{2}
		\]
		es una combinación lineal de variables aleatorias gaussianas normalizadas.
		Por la proposición \vref{prop:Gauss suma}, se tiene:
		\[
		\mathring{Z}\coloneqq a\sigma_{1}\left(\frac{\mathring{x}_{1}-\mu_{1}}{\sigma_{1}}\right)+b\sigma_{2}\left(\frac{\mathring{x}_{2}-\mu_{2}}{\sigma_{2}}\right)\sim\mathcal{N}\left(0,a^{2}\sigma_{1}^{2}+b^{2}\sigma_{2}^{2}\right)
		\]
		Sustituyendo de nuevo en la ecuación \vref{eq:Suma gauss y}, obtenemos:
		\[
		\mathring{y}=a\mu_{1}+b\mu_{2}+\mathring{Z}
		\]
		donde $\mathring{Z}\sim\mathcal{N}\left(0,a^{2}\sigma_{1}^{2}+b^{2}\sigma_{2}^{2}\right)$.
		Por la proposición \vref{prop:gaussiana + constante}, se tiene que:
		\[
		\mathring{y}\sim\mathcal{N}\left(a\mu_{1}+b\mu_{2},a^{2}\sigma_{1}^{2}+b^{2}\sigma_{2}^{2}\right)
		\]
	\end{proof}
	\begin{cor}[\textbf{¡Importante!}]
		\label{cor:Suma Gauss}Cualquier combinación lineal de variables
		aleatorias gaussianas es otra variable aleatoria gaussiana.
		
		\[
		\mathring{x}_{i}\sim\mathcal{N}\left(\mu_{i},\sigma_{i}^{2}\right)\quad\forall i=1,\dots,n\implies\mathring{y}=\sum_{i=1}^{n}\alpha_{i}\mathring{x}_{i}\sim\mathcal{N}\left(\sum_{i=1}^{n}\alpha_{i}\mu_{i},\sum_{i=1}^{n}\alpha_{i}^{2}\sigma_{i}^{2}\right)
		\]
		donde $\vec{\alpha}=\left(\alpha_{1},\dots,\alpha_{n}\right)\in\mathbb{R}^{n}$.
	\end{cor}
	\begin{proof}
		Probaremos el resultado por inducción sobre $n$. Por el corolario
		\vref{cor:Suma gauss 2}, el resultado se cumple para $n=2$. Supongamos
		que se cumple para $n$ y veamos si se cumple para $n+1$. Es decir,
		sean $\left\{ \mathring{x}_{i}\right\} _{i=1}^{n+1}$ variables aleatorias
		tales que:
		\[
		\mathring{x}_{i}\sim\mathcal{N}\left(\mu_{i},\sigma_{i}^{2}\right)\qquad\forall i=1,\dots,n
		\]
		y sea $\alpha\in\mathbb{R}^{n+1}$. Entonces:
		\[
		\mathring{y}=\sum_{i=1}^{n+1}\alpha_{i}\mathring{x}_{i}=\sum_{i=1}^{n}\alpha_{i}\mathring{x}_{i}+\alpha_{n+1}\mathring{x}_{n+1}
		\]
		Por hipótesis de inducción:
		\[
		\mathring{z}\coloneqq\sum_{i=1}^{n}\alpha_{i}\mathring{x}_{i}\sim\mathcal{N}\left(\sum_{i=1}^{n}\alpha_{i}\mu_{i},\sum_{i=1}^{n}\alpha_{i}^{2}\sigma_{i}^{2}\right)
		\]
		Entonces, nos queda por evaluar la suma de dos variables gaussianas:
		\[
		\mathring{y}=\mathring{z}+\alpha_{n+1}\mathring{x}_{n+1}
		\]
		Por el corolario \vref{cor:Suma gauss 2}, se tiene:
		\[
		\mathring{y}\sim\mathcal{N}\left(\sum_{i=1}^{n}\alpha_{i}\mu_{i}+\alpha_{n+1}\mathring{x}_{n+1},\sum_{i=1}^{n}\alpha_{i}^{2}\sigma_{i}^{2}+\alpha_{n+1}^{2}\sigma_{n+1}^{2}\right)=
		\]
		\[
		=\mathcal{N}\left(\sum_{i=1}^{n+1}\alpha_{i}\mu_{i},\sum_{i=1}^{n+1}\alpha_{i}^{2}\sigma_{i}^{2}\right)
		\]
		por lo que el resultado se cumple para $n+1$, lo que completa la
		demostración.
	\end{proof}
	
	\begin{cor}
		\label{cor:combinaci=0000F3n lineal gaussianas normalizadas}Toda
		combinación lineal normalizada de variables gaussianas normalizadas
		es otra variable aleatoria gaussiana normalizada.
		\[
		\left.\begin{matrix}\mathring{x}_{i}\sim\mathcal{N}\left(0,1\right)\quad\forall i=1,\dots,n\\
			\vec{\alpha}=\left(\alpha_{1},\dots,\alpha_{n}\right)\in\mathbb{R}^{n}\text{ t.q. }\left|\left|\vec{\alpha}\right|\right|=1
		\end{matrix}\right\} \implies\alpha^{T}\mathring{\vec{x}}=\sum_{i=1}^{n}\alpha_{i}\mathring{x}_{i}\sim\mathcal{N}\left(0,1\right)
		\]
	\end{cor}
	\begin{proof}
		Por el corolario \vref{cor:Suma gauss 2}, se tiene que:
		\[
		\sum_{i=1}^{n}\alpha_{i}\mathring{x}_{i}\sim\mathcal{N}\left(\sum_{i=1}^{n}\alpha_{i}\cdot0,\sum_{i=1}^{n}\alpha_{i}^{2}\cdot1^{2}\right)=\mathcal{N}\left(0,\sum_{i=1}^{n}\alpha_{i}^{2}\right)=
		\]
		\[
		=\mathcal{N}\left(0,\left|\left|\vec{\alpha}\right|\right|^{2}\right)=\mathcal{N}\left(0,1^{2}\right)=\mathcal{N}\left(0,1\right)
		\]
		dado que $\vec{\alpha}\in\mathbb{R}^{n}$ era de norma unidad.
	\end{proof}
	
	\subsection{Distribución $\chi_{\nu}^{2}$}
	\begin{prop}
		\label{prop:vector gaussiano chi cuadrado}Dado un vector de variables
		aleatorias gaussianas normalizadas $\mathring{\vec{x}}=\left(\mathring{x}_{1},\dots,\mathring{x}_{n}\right)$
		que pertenece a un espacio vectorial de dimensión $n-k$, entonces,
		el cuadrado de la norma euclídea del vector sigue una distribución
		$\chi^{2}$ con $n-k$ grados de libertad. Es decir:
		\[
		\left.\begin{matrix}\mathring{x}_{i}\sim\mathcal{N}\left(0,1\right)\quad\forall i=1,\dots,n\\
			\dim\left(\left\langle \mathring{\vec{x}}\right\rangle \right)=n-k
		\end{matrix}\right\} \implies\left|\left|\mathring{\vec{x}}\right|\right|^{2}=\mathring{x}_{1}^{2}+\dots+\mathring{x}_{n}^{2}=\sum_{i=1}^{n}\mathring{x}_{i}^{2}\sim\chi_{n-k}^{2}
		\]
	\end{prop}
	\begin{proof}
		Llamemos $W$ al espacio vectorial generado por el vector aleatorio
		$\mathring{\vec{x}}$, que tiene, por hipótesis, dimensión $n-k$.
		Entonces, sabemos que podemos descomponer $\mathbb{R}^{n}$ como suma
		directa de $W$ y de su complementario ortogonal. Es decir:
		\begin{equation}
			\mathbb{R}^{n}=W\oplus W^{\perp}\label{eq:LEO descomp R^n}
		\end{equation}
		A continuación, sea $\left\{ \vec{a}_{1},\dots,\vec{a}_{n-k}\right\} $
		una base ortonormal de $W$ (que sabemos que siempre existe al menos
		una) y sea $\left\{ \vec{b}_{1},\dots,\vec{b}_{k}\right\} $ una base
		ortonormal de $W^{\perp}$. Entonces, por la ecuación \vref{eq:LEO descomp R^n}
		$U\coloneqq\left\{ \vec{a}_{1},\dots,\vec{a}_{n-k}\right\} \cup\left\{ \vec{b}_{1},\dots,\vec{b}_{k}\right\} =\left\{ \vec{a}_{1},\dots,\vec{a}_{n-k},\vec{b}_{1},\dots,\vec{b}_{k}\right\} $
		es una base ortonormal de $\mathbb{R}^{n}$.
		
		Ahora bien, en este momento, estamos expresando el vector $\mathring{\vec{x}}$
		en base canónica ($E$), que es una base ortonormal. Por tanto, el
		cambio de base entre $E$ y $U$ vendrá dado por una matriz ortogonal.
		Es decir, existe $Q\in\mathbb{R}^{n\times n}$ tal que $Q^{T}Q=\mathbb{I}$
		y $\mathring{\vec{z}}\in\mathbb{R}^{n}$ tal que:
		\begin{equation}
			\mathring{\vec{z}}=Q\mathring{\vec{x}}\label{eq:LEO z}
		\end{equation}
		es la representación de $\mathring{\vec{x}}$ en base $U$. Además,
		como $Q$ es una matriz ortogonal, sus columnas son vectores de norma
		unidad de $\mathbb{R}^{n}$. De esta forma, cada $\mathring{z}_{i}$
		es una combinación lineal normalizada de las $\left\{ \mathring{x}_{i}\right\} _{i=1}^{n}$
		y, por el lema \vref{cor:combinaci=0000F3n lineal gaussianas normalizadas},
		cada $\mathring{z}_{i}$ es también una variable aleatoria gaussiana
		normalizada. Como $\mathring{\vec{x}}\in W$, sabemos que las últimas
		$k$ componentes de $\mathring{\vec{z}}$ serán, necesariamente, nulas.
		Es decir:
		\begin{equation}
			\mathring{z}_{n-k+1}=\mathring{z}_{n-k+2}=\dots=\mathring{z}_{n}=0\label{eq:LEO zetas nulos}
		\end{equation}
		
		Por otra parte:
		\[
		\left|\left|\mathring{\vec{x}}\right|\right|^{2}=\sum_{i=1}^{n}\mathring{x}_{i}^{2}=\mathring{\vec{x}}^{T}\mathring{\vec{x}}=\mathring{\vec{x}}^{T}\mathbb{I}\mathring{\vec{x}}=\mathring{\vec{x}}^{T}Q^{T}Q\mathring{\vec{x}}
		\]
		ya que $Q^{T}Q=\mathbb{I}$. Si seguimos operando, se tiene:
		\[
		\left|\left|\mathring{\vec{x}}\right|\right|^{2}=\left(Q\mathring{\vec{x}}\right)^{T}Q\mathring{\vec{x}}=\mathring{\vec{z}}^{T}\mathring{\vec{z}}=\sum_{i=1}^{n}\mathring{z}_{i}^{2}
		\]
		ya que era $\mathring{\vec{z}}=Q\mathring{\vec{x}}$ por la ecuación
		\vref{eq:LEO z}. Ahora bien, aplicando la ecuación \vref{eq:LEO zetas nulos},
		se obtiene:
		\[
		\left|\left|\mathring{\vec{x}}\right|\right|_{2}^{2}=\sum_{i=1}^{n-k}\mathring{z}_{i}^{2}+\sum_{i=n-k+1}^{n}\mathring{z}_{i}^{2}=\sum_{i=1}^{n-k}\mathring{z}_{i}^{2}
		\]
		Como $\left\{ \mathring{z}_{i}\right\} _{i=1}^{n-k}$ es un conjunto
		de $n-k$ variables aleatorias gaussianas normalizadas independientes
		entre sí, entonces:
		\[
		\left|\left|\mathring{\vec{x}}\right|\right|_{2}^{2}=\sum_{i=1}^{n}\mathring{x}_{i}^{2}=\sum_{i=1}^{n-k}\mathring{z}_{i}\sim\chi_{n-k}^{2}
		\]
		por definición de la distribución $\chi_{n-k}^{2}$.
	\end{proof}
	
	
	
	\section{Propagación de errores}\label{subsec:propagacion errores}
	\begin{thm}
		\label{thm:propagaci=0000F3n de errores}Sea $\Omega$ un conjunto
		abierto de $\mathbb{R}^{n}$. A continuación, sea $\mathring{\vec{x}}=\left(\mathring{x}_{1},\dots,\mathring{x}_{n}\right)$
		un vector de variables aleatorias y sea $f:\Omega\subseteq\mathbb{R}^{n}\longrightarrow\mathbb{R}$
		una función escalar de clase $C^{\left(1\right)}$. Al primer orden
		no nulo, se tiene:
		\[
		\mathrm{E}\left(f\left(\mathring{\vec{x}}\right)\right)=f\left(\mathrm{E}\left(\mathring{\vec{x}}\right)\right)+o\left(2\right)
		\]
		\[
		\mathrm{Var}\left(f\left(\mathring{\vec{x}}\right)\right)=\left[\vec{\nabla}f\left(\mathrm{E}\left(\mathring{\vec{x}}\right)\right)\right]^{T}\mathrm{Cov}\left(\mathring{\vec{x}}\right)\vec{\nabla}f\left(\mathrm{E}\left(\mathring{\vec{x}}\right)\right)+o\left(3\right)
		\]
	\end{thm}
	\begin{proof}
		Como la función $f$ es de clase $C^{\left(1\right)}$, admite un
		desarrollo de Taylor de orden $1$ en torno a $\mathrm{E}\left(\mathring{\vec{x}}\right)$.
		\begin{equation}
			f\left(\dot{\vec{x}}\right)=f\left(\mathrm{E}\left(\mathring{\vec{x}}\right)\right)+\vec{\nabla}f\left(\mathrm{E}\left(\mathring{\vec{x}}\right)\right)\cdot\left[\dot{\vec{x}}-\mathrm{E}\left(\mathring{\vec{x}}\right)\right]+o\left(2\right)\label{eq:f primer orden}
		\end{equation}
		donde es importante distinguir $\dot{\vec{x}}$ de $\mathring{\vec{x}}$.
		A continuación, obtengamos el valor esperado de $f\left(\mathring{\vec{x}}\right)$:
		\[
		\mathrm{E}\left[f\left(\mathring{\vec{x}}\right)\right]=\mathrm{E}\left[f\left(\mathrm{E}\left(\mathring{\vec{x}}\right)\right)+\vec{\nabla}f\left(\mathrm{E}\left(\mathring{\vec{x}}\right)\right)\cdot\left[\mathring{\vec{x}}-\mathrm{E}\left(\mathring{\vec{x}}\right)\right]+o\left(2\right)\right]=
		\]
		\[
		=f\left(\mathrm{E}\left(\mathring{\vec{x}}\right)\right)+\vec{\nabla}f\left(\mathrm{E}\left(\mathring{\vec{x}}\right)\right)\mathrm{E}\left(\mathring{\vec{x}}-\mathrm{E}\left(\mathring{\vec{x}}\right)\right)+o\left(2\right)
		\]
		Ahora bien, recordemos que $\mathrm{E}\left(\mathring{\vec{x}}-\mathrm{E}\left(\mathring{\vec{x}}\right)\right)=0$
		para cualquier variable aleatoria. De esta forma, tenemos:
		\[
		\mathrm{E}\left[f\left(\mathring{\vec{x}}\right)\right]=f\left(\mathrm{E}\left(\mathring{\vec{x}}\right)\right)+o\left(2\right)
		\]
		con lo que llegamos a la primera parte del enunciado.
		
		Vamos con la varianza. Aplicaremos la expresión:
		\[
		\mathrm{Var}\left(\mathring{y}\right)=\mathrm{E}\left(\mathring{y}^{2}\right)-\left(\mathrm{E}\left(\mathring{y}\right)\right)^{2}
		\]
		a la variable aleatoria $\mathring{y}=f\left(\mathring{\vec{x}}\right)$,
		que sabemos que debe cumplirse para cualquier variable aleatoria.
		Elevando al cuadrado la expresión \vref{eq:f primer orden}, se tiene:
		\[
		f\left(\dot{\vec{x}}\right)^{2}=f\left(\mathrm{E}\left(\mathring{\vec{x}}\right)\right)^{2}+\left[\vec{\nabla}f\left(\mathrm{E}\left(\mathring{\vec{x}}\right)\right)\cdot\left(\dot{\vec{x}}-\mathrm{E}\left(\mathring{\vec{x}}\right)\right)\right]^{2}+2f\left(\mathrm{E}\left(\mathring{\vec{x}}\right)\right)\vec{\nabla}f\left(\mathrm{E}\left(\mathring{\vec{x}}\right)\right)\cdot\left(\dot{\vec{x}}-\mathrm{E}\left(\mathring{\vec{x}}\right)\right)+o\left(3\right)
		\]
		agrupando en $o\left(3\right)$ todos los términos de orden $3$ y
		superior. A continuación, podemos calcular el valor esperado de $\left[f\left(\mathring{\vec{x}}\right)\right]^{2}$:
		\[
		\mathrm{E}\left[f\left(\mathring{\vec{x}}\right)^{2}\right]=\mathrm{E}\left[f\left(\mathrm{E}\left(\mathring{\vec{x}}\right)\right)^{2}+\left[\vec{\nabla}f\left(\mathrm{E}\left(\mathring{\vec{x}}\right)\right)\cdot\left(\mathring{\vec{x}}-\mathrm{E}\left(\mathring{\vec{x}}\right)\right)\right]^{2}+\right.
		\]
		\[
		\left.+2f\left(\mathrm{E}\left(\mathring{\vec{x}}\right)\right)\vec{\nabla}f\left(\mathrm{E}\left(\mathring{\vec{x}}\right)\right)\cdot\left(\mathring{\vec{x}}-\mathrm{E}\left(\mathring{\vec{x}}\right)\right)+o\left(3\right)\right]=
		\]
		\[
		=f\left(\mathrm{E}\left(\mathring{\vec{x}}\right)\right)^{2}+\mathrm{E}\left[\left(\vec{\nabla}f\left(\mathrm{E}\left(\mathring{\vec{x}}\right)\right)\cdot\left(\mathring{\vec{x}}-\mathrm{E}\left(\mathring{\vec{x}}\right)\right)\right)^{2}\right]+2f\left(\mathrm{E}\left(\mathring{\vec{x}}\right)\right)\vec{\nabla}f\left(\mathrm{E}\left(\mathring{\vec{x}}\right)\right)\cdot\mathrm{E}\left(\mathring{\vec{x}}-\mathrm{E}\left(\mathring{\vec{x}}\right)\right)+o\left(3\right)
		\]
		De nuevo, como es $\mathrm{E}\left(\mathring{\vec{x}}-\mathrm{E}\left(\mathring{\vec{x}}\right)\right)=0$,
		se tiene:
		\[
		\mathrm{E}\left[f\left(\mathring{\vec{x}}\right)^{2}\right]=f\left(\mathrm{E}\left(\mathring{\vec{x}}\right)\right)^{2}+\mathrm{E}\left[\left(\vec{\nabla}f\left(\mathrm{E}\left(\mathring{\vec{x}}\right)\right)\cdot\left(\mathring{\vec{x}}-\mathrm{E}\left(\mathring{\vec{x}}\right)\right)\right)^{2}\right]+o\left(3\right)
		\]
		De esta forma, ya estamos en disposición de obtener la varianza de
		$f\left(\mathring{\vec{x}}\right)$:
		\[
		\mathrm{Var}\left(f\left(\mathring{\vec{x}}\right)\right)=\mathrm{E}\left(f\left(\mathring{\vec{x}}\right)^{2}\right)-\left[\mathrm{E}\left(f\left(\mathring{\vec{x}}\right)\right)\right]^{2}=
		\]
		\[
		=f\left(\mathrm{E}\left(\mathring{\vec{x}}\right)\right)^{2}+\mathrm{E}\left[\left(\vec{\nabla}f\left(\mathrm{E}\left(\mathring{\vec{x}}\right)\right)\cdot\left(\mathring{\vec{x}}-\mathrm{E}\left(\mathring{\vec{x}}\right)\right)\right)^{2}\right]-f\left(\mathrm{E}\left(\mathring{\vec{x}}\right)\right)^{2}+o\left(3\right)=
		\]
		\[
		=\mathrm{E}\left[\left(\vec{\nabla}f\left(\mathrm{E}\left(\mathring{\vec{x}}\right)\right)\cdot\left(\mathring{\vec{x}}-\mathrm{E}\left(\mathring{\vec{x}}\right)\right)\right)^{2}\right]+o\left(3\right)
		\]
		Para proseguir, desarrollamos el producto escalar dentro del cuadrado:
		\[
		\mathrm{Var}\left(f\left(\mathring{\vec{x}}\right)\right)=\mathrm{E}\left[\left(\sum_{i=1}^{n}\frac{\partial f}{\partial\dot{x}_{i}}\left(\mathrm{E}\left(\mathring{\vec{x}}\right)\right)\left(\mathring{x}_{i}-\mathrm{E}\left(\mathring{x}_{i}\right)\right)\right)^{2}\right]+o\left(3\right)
		\]
		Ahora, expandimos el cuadrado anterior como el doble sumatorio de
		cualquier combinación de términos:
		\[
		\mathrm{Var}\left(f\left(\mathring{\vec{x}}\right)\right)=\mathrm{E}\left[\sum_{i=1}^{n}\sum_{j=1}^{n}\frac{\partial f}{\partial\dot{x}_{i}}\left(\mathrm{E}\left(\mathring{\vec{x}}\right)\right)\frac{\partial f}{\partial\dot{x}_{j}}\left(\mathrm{E}\left(\mathring{\vec{x}}\right)\right)\left(\mathring{x}_{i}-\mathrm{E}\left(\mathring{x}_{i}\right)\right)\left(\mathring{x}_{j}-\mathrm{E}\left(\mathring{x}_{j}\right)\right)\right]+o\left(3\right)=
		\]
		\[
		=\sum_{i=1}^{n}\sum_{j=1}^{n}\frac{\partial f}{\partial\dot{x}_{i}}\left(\mathrm{E}\left(\mathring{\vec{x}}\right)\right)\frac{\partial f}{\partial\dot{x}_{j}}\left(\mathrm{E}\left(\mathring{\vec{x}}\right)\right)\mathrm{E}\left[\left(\mathring{x}_{i}-\mathrm{E}\left(\mathring{x}_{i}\right)\right)\left(\mathring{x}_{j}-\mathrm{E}\left(\mathring{x}_{j}\right)\right)\right]+o\left(3\right)
		\]
		Justo el término $\mathrm{E}\left[\left(\mathring{x}_{i}-\mathrm{E}\left(\mathring{x}_{i}\right)\right)\left(\mathring{x}_{j}-\mathrm{E}\left(\mathring{x}_{j}\right)\right)\right]$
		es la definición de covarianza entre las variables aleatorias $\mathring{x}_{i}$
		y $\mathring{x}_{j}$. De esta forma, obtenemos:
		\[
		\mathrm{Var}\left(f\left(\mathring{\vec{x}}\right)\right)=\sum_{i=1}^{n}\sum_{j=1}^{n}\frac{\partial f}{\partial\dot{x}_{i}}\left(\mathrm{E}\left(\mathring{\vec{x}}\right)\right)\frac{\partial f}{\partial\dot{x}_{j}}\left(\mathrm{E}\left(\mathring{\vec{x}}\right)\right)\mathrm{Cov}\left(\mathring{x}_{i},\mathring{x}_{j}\right)+o\left(3\right)
		\]
		A continuación, es nuestra intención convertir lo anterior en un producto
		vector{*}matriz{*}vector. Para ello, recordamos:
		\[
		\vec{\nabla}f=\left(\frac{\partial f}{\partial\dot{x}_{1}},\dots,\frac{\partial f}{\partial\dot{x}_{n}}\right)
		\]
		\[
		\mathrm{Cov}\left(\mathring{\vec{x}}\right)=\left(\begin{matrix}\mathrm{Var}\left(\mathring{x}_{1}\right) & \mathrm{Cov}\left(\mathring{x}_{1},\mathring{x}_{2}\right) & \cdots & \mathrm{Cov}\left(\mathring{x}_{1},\mathring{x}_{n-1}\right) & \mathrm{Cov}\left(\mathring{x}_{1},\mathring{x}_{n}\right)\\
			\mathrm{Cov}\left(\mathring{x}_{2},\mathring{x}_{1}\right) & \ddots &  &  & \mathrm{Cov}\left(\mathring{x}_{2},\mathring{x}_{n}\right)\\
			\vdots &  & \ddots &  & \vdots\\
			\mathrm{Cov}\left(\mathring{x}_{n-1},\mathring{x}_{1}\right) &  &  & \ddots & \mathrm{Cov}\left(\mathring{x}_{n-1},\mathring{x}_{n}\right)\\
			\mathrm{Cov}\left(\mathring{x}_{n},\mathring{x}_{1}\right) & \mathrm{Cov}\left(\mathring{x}_{n},\mathring{x}_{2}\right) & \cdots & \mathrm{Cov}\left(\mathring{x}_{n},\mathring{x}_{n-1}\right) & \mathrm{Var}\left(\mathring{x}_{n}\right)
		\end{matrix}\right)
		\]
		Con esto en mente, es fácil ver que:
		\[
		\mathrm{Var}\left(f\left(\mathring{\vec{x}}\right)\right)=\sum_{i=1}^{n}\sum_{j=1}^{n}\left(\vec{\nabla}f\left(\mathrm{E}\left(\mathring{\vec{x}}\right)\right)\right)_{i}\left(\vec{\nabla}f\left(\mathrm{E}\left(\mathring{\vec{x}}\right)\right)\right)_{j}\left(\mathrm{Cov}\left(\mathring{\vec{x}}\right)\right)_{i,j}+o\left(3\right)=
		\]
		\[
		=\sum_{i=1}^{n}\left(\vec{\nabla}f\left(\mathrm{E}\left(\mathring{\vec{x}}\right)\right)\right)_{i}\left(\sum_{j=1}^{n}\left(\mathrm{Cov}\left(\mathring{\vec{x}}\right)\right)_{i,j}\left(\vec{\nabla}f\left(\mathrm{E}\left(\mathring{\vec{x}}\right)\right)\right)_{j}\right)+o\left(3\right)=
		\]
		\[
		=\sum_{i=1}^{n}\left(\vec{\nabla}f\left(\mathrm{E}\left(\mathring{\vec{x}}\right)\right)\right)_{i}\left(\mathrm{Cov}\left(\mathring{\vec{x}}\right)\vec{\nabla}f\left(\mathrm{E}\left(\mathring{\vec{x}}\right)\right)\right)_{i}=\left[\vec{\nabla}f\left(\mathrm{E}\left(\mathring{\vec{x}}\right)\right)\right]^{T}\mathrm{Cov}\left(\mathring{\vec{x}}\right)\vec{\nabla}f\left(\mathrm{E}\left(\mathring{\vec{x}}\right)\right)+o\left(3\right)
		\]
		donde hemos usado los subíndices $i$ y $j$ para indicar los elementos
		de las matrices o vectores correspondientes y $T$ indica <<transpuesta>>.
	\end{proof}
	\begin{cor}
		Sea $\Omega$ un abierto de $\mathbb{R}^{n}$. A continuación, sea
		$\mathring{\vec{x}}=\left(\mathring{x}_{1},\dots,\mathring{x}_{n}\right)$
		un vector de variables aleatorias independientes dos a dos y sea $f:\Omega\subseteq\mathbb{R}^{n}\longrightarrow\mathbb{R}$
		una función escalar de clase $C^{\left(1\right)}$. Al primer orden
		no nulo, se tiene:
		\[
		\mathrm{E}\left[f\left(\mathring{\vec{x}}\right)\right]=f\left(\mathrm{E}\left(\mathring{\vec{x}}\right)\right)+o\left(2\right)
		\]
		\[
		\mathrm{Var}\left[f\left(\mathring{\vec{x}}\right)\right]=\sum_{i=1}^{n}\left[\frac{\partial f}{\partial\dot{x}_{i}}\left(\mathrm{E}\left(\mathring{\vec{x}}\right)\right)\right]^{2}\mathrm{Var}\left(\mathring{x}_{i}\right)+o\left(3\right)
		\]
	\end{cor}
	\begin{proof}
		El primer resultado se obtiene directamente del teorema \vref{thm:propagaci=0000F3n de errores}.
		Por ese mismo teorema se tiene:
		\[
		\mathrm{Var}\left(f\left(\mathring{\vec{x}}\right)\right)=\left[\vec{\nabla}f\left(\mathrm{E}\left(\mathring{\vec{x}}\right)\right)\right]^{T}\mathrm{Cov}\left(\mathring{\vec{x}}\right)\vec{\nabla}f\left(\mathrm{E}\left(\mathring{\vec{x}}\right)\right)+o\left(3\right)
		\]
		En nuestro caso, como las variables $\mathring{\vec{x}}=\left(\mathring{x}_{1},\dots,\mathring{x}_{n}\right)$
		son independientes dos a dos, se tiene:
		\[
		\mathrm{Cov}\left(\mathring{x}_{i},\mathring{x}_{j}\right)=0\qquad\forall i\neq j\;\land\;i,j=1,\dots,n
		\]
		Por tanto, la matriz $\mathrm{Cov}\left(\mathring{\vec{x}}\right)$
		es diagonal. De esta forma, el producto vector{*}matriz{*}vector queda,
		sencillamente:
		\[
		\mathrm{Var}\left(f\left(\mathring{\vec{x}}\right)\right)=\sum_{i=1}^{n}\sum_{j=1}^{n}\left(\vec{\nabla}f\left(\mathrm{E}\left(\mathring{\vec{x}}\right)\right)\right)_{i}\left(\mathrm{Cov}\left(\mathring{\vec{x}}\right)\right)_{i,j}\left(\vec{\nabla}f\left(\mathrm{E}\left(\mathring{\vec{x}}\right)\right)\right)_{j}=
		\]
		\[
		=\sum_{i=1}^{n}\left[\left(\vec{\nabla}f\left(\mathrm{E}\left(\mathring{\vec{x}}\right)\right)\right)_{i}\right]^{2}\left(\mathrm{Cov}\left(\mathring{\vec{x}}\right)\right)_{i,i}
		\]
		ya que $\mathrm{Cov}\left(\mathring{x}_{i},\mathring{x}_{j}\right)=0\;\forall i\neq j$.
		De esta forma, recordando que $\left(\mathrm{Cov}\left(\mathring{\vec{x}}\right)\right)_{i,i}=\mathrm{Var}\left(\mathring{x}_{i}\right)$
		y que $\left(\vec{\nabla}f\right)_{i}=\frac{\partial f}{\partial\dot{x}_{i}}$,
		se llega a:
		\[
		\mathrm{Var}\left(f\left(\mathring{\vec{x}}\right)\right)=\sum_{i=1}^{n}\left[\frac{\partial f}{\partial\dot{x}_{i}}\left(\mathrm{E}\left(\mathring{\vec{x}}\right)\right)\right]^{2}\mathrm{Var}\left(\mathring{x}_{i}\right)
		\]
	\end{proof}
	
	
	\section{Ajustes lineales con errores únicamente en las ordenadas}\label{subsec:LEO}
	\begin{assumption}
		\label{assu:LEO xi exactos}Los datos medidos $\left\{ x_{i}\right\} _{i=1}^{n}$
		son exactos.
	\end{assumption}
	%
	\begin{assumption}
		\label{assu:LEO yi}Cada dato medido $y_{i}$ es una muestra de una
		distribución gaussiana de centro <<el valor real>> $\tilde{y}_{i}$
		desconocido y de varianza $\sigma_{i}^{2}$ conocida, que denotaremos
		$\mathring{y}_{i}$. Es decir:
		\[
		\mathring{y}_{i}\sim\mathcal{N}\left(\tilde{y}_{i},\sigma_{i}^{2}\right)\qquad\forall i=1,\dots,n
		\]
		Además, las variables aleatorias asociadas a los datos medidos $\left\{ \mathring{y}_{i}\right\} _{i=1}^{n}$
		son independientes dos a dos.
	\end{assumption}
	%
	\begin{assumption}
		\label{assu:LEO yi tilde}Los valores reales $\left\{ \tilde{y}_{i}\right\} _{i=1}^{n}$
		satisfacen la ecuación:
		\[
		\tilde{y}_{i}=ax_{i}+b\qquad\forall i=1,\dots,n
		\]
		donde $a,b\in\mathbb{R}$ son dos constantes (los parámetros del modelo).
	\end{assumption}
	\begin{prop}
		\label{prop:LEO m=0000E1xima verosimliitud}Los estimadores de máxima
		verosimilitud para los parámetros $a$ y $b$ del modelo vienen dados
		por:
		\[
		\hat{a}=\frac{{\displaystyle \left(\sum_{i=1}^{n}\frac{x_{i}}{\sigma_{i}^{2}}\right)\left(\sum_{i=1}^{n}\frac{y_{i}}{\sigma_{i}^{2}}\right)-\left(\sum_{i=1}^{n}\frac{1}{\sigma_{i}^{2}}\right)\left(\sum_{i=1}^{n}\frac{x_{i}y_{i}}{\sigma_{i}^{2}}\right)}}{{\displaystyle \left(\sum_{i=1}^{n}\frac{x_{i}}{\sigma_{i}^{2}}\right)^{2}-\left(\sum_{i=1}^{n}\frac{1}{\sigma_{i}^{2}}\right)\left(\sum_{i=1}^{n}\frac{x_{i}^{2}}{\sigma_{i}^{2}}\right)}}
		\]
		\[
		\hat{b}=\frac{{\displaystyle \left(\sum_{i=1}^{n}\frac{y_{i}}{\sigma_{i}^{2}}\right)-\left(\sum_{i=1}^{n}\frac{x_{i}}{\sigma_{i}^{2}}\right)\hat{a}}}{{\displaystyle \left(\sum_{i=1}^{n}\frac{1}{\sigma_{i}^{2}}\right)}}
		\]
	\end{prop}
	\begin{proof}
		Por la suposición \vref{assu:LEO yi}, sabemos que los datos medidos
		$\left\{ y_{i}\right\} _{i=1}^{n}$ son muestras de variables aleatorias
		gaussianas de centro $\tilde{y}_{i}$ y desviación típica $\sigma_{i}$.
		Es decir:
		\[
		\mathring{y}_{i}\sim\mathcal{N}\left(\tilde{y}_{i},\sigma_{i}^{2}\right)\qquad\forall i=1,\dots,n
		\]
		Ahora bien, por la suposición \vref{assu:LEO yi tilde}, se tiene
		$\tilde{y}_{i}=ax_{i}+b$. Entonces, las variables aleatorias $\left\{ \mathring{y}_{i}\right\} _{i=1}^{n}$
		siguen distribuciones normales de centro $ax_{i}+b$ y de desviación
		típica $\sigma_{i}$. Dicho de otra forma:
		\[
		\mathring{y}_{i}\sim\mathcal{N}\left(ax_{i}+b,\sigma_{i}^{2}\right)\qquad\forall i=1,\dots,n
		\]
		Sabemos que lo anterior está bien definido ya que los $\left\{ x_{i}\right\} _{i=1}^{n}$
		son exactos por la suposición \vref{assu:LEO xi exactos}.
		
		De esta manera, la función de densidad de probabilidad de cada variable
		aleatoria $\mathring{y}_{i}$ viene dada por:
		\[
		f_{\mathring{y}_{i}}\left(\dot{y}_{i}\right)=\frac{1}{\sqrt{2\pi\sigma_{i}^{2}}}\exp\left(-\frac{\left(\dot{y}_{i}-ax_{i}-b\right)^{2}}{2\sigma_{i}^{2}}\right)\qquad\forall i=1,\dots,n
		\]
		donde no debe confundirse la variable aleatoria $\mathring{y}_{i}$
		con el argumento de la función de densidad de probabilidad $f_{\mathring{y}_{i}}$:
		$\dot{y}_{i}$. 
		
		A continuación como, por la suposición \vref{assu:LEO yi}, las variables
		aleatorias $\left\{ \mathring{y}_{i}\right\} _{i=1}^{n}$ son independientes
		dos a dos, la función de verosimilitud será simplemente el producto
		de las funciones de densidad $f_{\mathring{y}_{i}}$ evaluadas en
		los datos medidos:
		\[
		L\left(a,b\right)=\prod_{i=1}^{n}f_{\mathring{y}_{i}}\left(\dot{y}_{i}=y_{i}\right)=\frac{1}{\sqrt{2^{n}\pi^{n}\prod_{i=1}^{n}\sigma_{i}^{2}}}\exp\left(-\sum_{i=1}^{n}\left(\frac{y_{i}-ax_{i}-b}{\sqrt{2}\sigma_{i}}\right)^{2}\right)
		\]
		donde hemos usado la propiedad $\mathrm{e}^{a}\mathrm{e}^{b}=\mathrm{e}^{a+b}\;\forall a,b\in\mathbb{R}$.
		Para proseguir, consideramos el logaritmo de la función de verosimilitud:
		\[
		l\left(a,b\right)=\ln L=\ln\left(\frac{1}{\sqrt{2^{n}\pi^{n}\prod_{i=1}^{n}\sigma_{i}^{2}}}\right)-\sum_{i=1}^{n}\left(\frac{y_{i}-ax_{i}-b}{\sqrt{2}\sigma_{i}}\right)^{2}
		\]
		donde hemos hecho uso de la propiedad del logaritmo de transformar
		productos en sumas y hemos empleado, asimismo, que la exponencial
		y el logaritmo son operaciones inversas.
		
		Con el fin de obtener los operadores de máxima verosimilitud, debemos
		buscar aquellos puntos en los que $l$ es máxima (que ya sabemos que
		coincidirán con los de $L$ por ser el logaritmo neperiano una función
		estrictamente creciente). Para ello, derivamos la función $l$ con
		respecto a los parámetros $a$ y $b$:
		\begin{equation}
			\frac{\partial l}{\partial a}=-\sum_{i=1}^{n}2\frac{y_{i}-ax_{i}-b}{\sqrt{2}\sigma_{i}}\left(-\frac{x_{i}}{\sqrt{2}\sigma_{i}}\right)=\sum_{i=1}^{n}x_{i}\frac{y_{i}-ax_{i}-b}{\sigma_{i}^{2}}=\sum_{i=1}^{n}x_{i}\frac{y_{i}-ax_{i}-b}{\sigma_{i}^{2}}\label{eq:LEO partial a}
		\end{equation}
		\begin{equation}
			\frac{\partial l}{\partial b}=-\sum_{i=1}^{n}2\frac{y_{i}-ax_{i}-b}{\sqrt{2}\sigma_{i}}\left(-\frac{1}{\sqrt{2}\sigma_{i}}\right)=\sum_{i=1}^{n}\frac{y_{i}-ax_{i}-b}{\sigma_{i}^{2}}=\sum_{i=1}^{n}\frac{y_{i}-ax_{i}-b}{\sigma_{i}^{2}}\label{eq:LEO partial b}
		\end{equation}
		donde hemos hecho uso de que la derivada parcial es una aplicación
		lineal y de la regla de la cadena. Ahora, imponemos $\vec{\nabla}l=\left(\frac{\partial l}{\partial a},\frac{\partial l}{\partial b}\right)=\vec{0}$
		para obtener el punto crítico de $l$:
		\[
		\frac{\partial l}{\partial b}=0\iff\sum_{i=1}^{n}x_{i}\frac{y_{i}-ax_{i}-b}{\sigma_{i}^{2}}=0
		\]
		\[
		\frac{\partial l}{\partial a}=0\iff\sum_{i=1}^{n}\frac{y_{i}-ax_{i}-b}{\sigma_{i}^{2}}=0
		\]
		Expandiendo las sumas anteriores y sacando los parámetros $a$ y $b$
		como factor común de los sumatorios, se tiene:
		\begin{equation}
			\frac{\partial l}{\partial b}=0\iff\left(\sum_{i=1}^{n}\frac{y_{i}}{\sigma_{i}^{2}}\right)-\left(\sum_{i=1}^{n}\frac{x_{i}}{\sigma_{i}^{2}}\right)a-\left(\sum_{i=1}^{n}\frac{1}{\sigma_{i}^{2}}\right)b=0\label{eq:LEO part b}
		\end{equation}
		\begin{equation}
			\frac{\partial l}{\partial a}=0\iff\left(\sum_{i=1}^{n}\frac{x_{i}y_{i}}{\sigma_{i}^{2}}\right)-\left(\sum_{i=1}^{n}\frac{x_{i}^{2}}{\sigma_{i}^{2}}\right)a-\left(\sum_{i=1}^{n}\frac{x_{i}}{\sigma_{i}^{2}}\right)b=0\label{eq:LEO part a}
		\end{equation}
		Nuestro objetivo es obtener los valores de $a$ y $b$ que resuelven
		el sistema de ecuaciones anterior. Para tal fin, aplicaremos el método
		de la reducción, buscando que el coeficiente de $b$ sea el mismo
		en ambas ecuaciones. Por consiguiente, multiplicamos la ecuación \vref{eq:LEO part b}
		por $\left(\sum_{i=1}^{n}\frac{x_{i}}{\sigma_{i}^{2}}\right)$ y la
		ecuación \vref{eq:LEO part a} por $\left(\sum_{i=1}^{n}\frac{1}{\sigma_{i}^{2}}\right)$.
		De esta forma, se obtiene:
		\begin{equation}
			\left(\sum_{i=1}^{n}\frac{x_{i}}{\sigma_{i}^{2}}\right)\left(\sum_{i=1}^{n}\frac{y_{i}}{\sigma_{i}^{2}}\right)-\left(\sum_{i=1}^{n}\frac{x_{i}}{\sigma_{i}^{2}}\right)^{2}a-\left(\sum_{i=1}^{n}\frac{x_{i}}{\sigma_{i}^{2}}\right)\left(\sum_{i=1}^{n}\frac{1}{\sigma_{i}^{2}}\right)b=0\label{eq:LEO part b 2}
		\end{equation}
		\begin{equation}
			\left(\sum_{i=1}^{n}\frac{1}{\sigma_{i}^{2}}\right)\left(\sum_{i=1}^{n}\frac{x_{i}y_{i}}{\sigma_{i}^{2}}\right)-\left(\sum_{i=1}^{n}\frac{1}{\sigma_{i}^{2}}\right)\left(\sum_{i=1}^{n}\frac{x_{i}^{2}}{\sigma_{i}^{2}}\right)a-\left(\sum_{i=1}^{n}\frac{x_{i}}{\sigma_{i}^{2}}\right)\left(\sum_{i=1}^{n}\frac{1}{\sigma_{i}^{2}}\right)b=0\label{eq:LEO part a 2}
		\end{equation}
		Nótese que, como estábamos buscando, ahora el coeficiente que acompaña
		a $b$ en ambas ecuaciones es el mismo. De esta forma, restándole
		a la ecuación \vref{eq:LEO part b 2} la ecuación \vref{eq:LEO part a 2},
		se tiene:
		\[
		\left(\sum_{i=1}^{n}\frac{x_{i}}{\sigma_{i}^{2}}\right)\left(\sum_{i=1}^{n}\frac{y_{i}}{\sigma_{i}^{2}}\right)-\left(\sum_{i=1}^{n}\frac{1}{\sigma_{i}^{2}}\right)\left(\sum_{i=1}^{n}\frac{x_{i}y_{i}}{\sigma_{i}^{2}}\right)-\left[\left(\sum_{i=1}^{n}\frac{x_{i}}{\sigma_{i}^{2}}\right)^{2}-\left(\sum_{i=1}^{n}\frac{1}{\sigma_{i}^{2}}\right)\left(\sum_{i=1}^{n}\frac{x_{i}^{2}}{\sigma_{i}^{2}}\right)\right]a=0
		\]
		ya que el término en $b$ desaparece. De la ecuación anterior resulta
		razonablemente sencillo despejar $a$:
		\[
		\left(\sum_{i=1}^{n}\frac{x_{i}}{\sigma_{i}^{2}}\right)\left(\sum_{i=1}^{n}\frac{y_{i}}{\sigma_{i}^{2}}\right)-\left(\sum_{i=1}^{n}\frac{1}{\sigma_{i}^{2}}\right)\left(\sum_{i=1}^{n}\frac{x_{i}y_{i}}{\sigma_{i}^{2}}\right)=\left[\left(\sum_{i=1}^{n}\frac{x_{i}}{\sigma_{i}^{2}}\right)^{2}-\left(\sum_{i=1}^{n}\frac{1}{\sigma_{i}^{2}}\right)\left(\sum_{i=1}^{n}\frac{x_{i}^{2}}{\sigma_{i}^{2}}\right)\right]a\iff
		\]
		\[
		\iff a=\frac{\left(\sum_{i=1}^{n}\frac{x_{i}}{\sigma_{i}^{2}}\right)\left(\sum_{i=1}^{n}\frac{y_{i}}{\sigma_{i}^{2}}\right)-\left(\sum_{i=1}^{n}\frac{1}{\sigma_{i}^{2}}\right)\left(\sum_{i=1}^{n}\frac{x_{i}y_{i}}{\sigma_{i}^{2}}\right)}{\left(\sum_{i=1}^{n}\frac{x_{i}}{\sigma_{i}^{2}}\right)^{2}-\left(\sum_{i=1}^{n}\frac{1}{\sigma_{i}^{2}}\right)\left(\sum_{i=1}^{n}\frac{x_{i}^{2}}{\sigma_{i}^{2}}\right)}
		\]
		Conocido el valor de $a$, podemos simplemente despejar $b$ de la
		ecuación \vref{eq:LEO part b}:
		\[
		\left(\sum_{i=1}^{n}\frac{y_{i}}{\sigma_{i}^{2}}\right)-\left(\sum_{i=1}^{n}\frac{x_{i}}{\sigma_{i}^{2}}\right)a-\left(\sum_{i=1}^{n}\frac{1}{\sigma_{i}^{2}}\right)b=0\iff\left(\sum_{i=1}^{n}\frac{y_{i}}{\sigma_{i}^{2}}\right)-\left(\sum_{i=1}^{n}\frac{x_{i}}{\sigma_{i}^{2}}\right)a=\left(\sum_{i=1}^{n}\frac{1}{\sigma_{i}^{2}}\right)b\iff
		\]
		\[
		\iff b=\frac{\left(\sum_{i=1}^{n}\frac{y_{i}}{\sigma_{i}^{2}}\right)-\left(\sum_{i=1}^{n}\frac{x_{i}}{\sigma_{i}^{2}}\right)a}{\left(\sum_{i=1}^{n}\frac{1}{\sigma_{i}^{2}}\right)}
		\]
		
		Por tanto, los estimadores de máxima verosimilitud para los operadores
		$a$ y $b$ serán:
		\[
		\hat{a}=\frac{\left(\sum_{i=1}^{n}\frac{x_{i}}{\sigma_{i}^{2}}\right)\left(\sum_{i=1}^{n}\frac{y_{i}}{\sigma_{i}^{2}}\right)-\left(\sum_{i=1}^{n}\frac{1}{\sigma_{i}^{2}}\right)\left(\sum_{i=1}^{n}\frac{x_{i}y_{i}}{\sigma_{i}^{2}}\right)}{\left(\sum_{i=1}^{n}\frac{x_{i}}{\sigma_{i}^{2}}\right)^{2}-\left(\sum_{i=1}^{n}\frac{1}{\sigma_{i}^{2}}\right)\left(\sum_{i=1}^{n}\frac{x_{i}^{2}}{\sigma_{i}^{2}}\right)}
		\]
		\[
		\hat{b}=\frac{\left(\sum_{i=1}^{n}\frac{y_{i}}{\sigma_{i}^{2}}\right)-\left(\sum_{i=1}^{n}\frac{x_{i}}{\sigma_{i}^{2}}\right)\hat{a}}{\left(\sum_{i=1}^{n}\frac{1}{\sigma_{i}^{2}}\right)}
		\]
	\end{proof}
	\begin{prop}
		Los estimadores $\hat{a}$ y $\hat{b}$ son no sesgados.
	\end{prop}
	\begin{proof}
		Recordemos que un operador $\hat{a}$ es no sesgado si $\mathrm{E}\left(\hat{a}\right)=a$.
		Para este cálculo, consideraremos que las muestras $y_{i}$ que aparecen
		en las expresiones de la proposición \vref{prop:LEO m=0000E1xima verosimliitud}
		representan sus variables aleatorias asociadas $\mathring{y}_{i}$.
		\[
		\hat{a}=\frac{\left(\sum_{i=1}^{n}\frac{x_{i}}{\sigma_{i}^{2}}\right)\left(\sum_{i=1}^{n}\frac{\mathring{y}_{i}}{\sigma_{i}^{2}}\right)-\left(\sum_{i=1}^{n}\frac{1}{\sigma_{i}^{2}}\right)\left(\sum_{i=1}^{n}\frac{x_{i}\mathring{y}_{i}}{\sigma_{i}^{2}}\right)}{\left(\sum_{i=1}^{n}\frac{x_{i}}{\sigma_{i}^{2}}\right)^{2}-\left(\sum_{i=1}^{n}\frac{1}{\sigma_{i}^{2}}\right)\left(\sum_{i=1}^{n}\frac{x_{i}^{2}}{\sigma_{i}^{2}}\right)}
		\]
		Como, por la suposición \vref{assu:LEO xi exactos}, los $\left\{ x_{i}\right\} _{i=1}^{n}$
		son datos exactos, se tiene:
		\[
		\mathrm{E}\left(\hat{a}\right)=\frac{\mathrm{E}\left[\left(\sum_{i=1}^{n}\frac{x_{i}}{\sigma_{i}^{2}}\right)\left(\sum_{i=1}^{n}\frac{\mathring{y}_{i}}{\sigma_{i}^{2}}\right)-\left(\sum_{i=1}^{n}\frac{1}{\sigma_{i}^{2}}\right)\left(\sum_{i=1}^{n}\frac{x_{i}\mathring{y}_{i}}{\sigma_{i}^{2}}\right)\right]}{\left(\sum_{i=1}^{n}\frac{x_{i}}{\sigma_{i}^{2}}\right)^{2}-\left(\sum_{i=1}^{n}\frac{1}{\sigma_{i}^{2}}\right)\left(\sum_{i=1}^{n}\frac{x_{i}^{2}}{\sigma_{i}^{2}}\right)}=
		\]
		\[
		=\frac{\left(\sum_{i=1}^{n}\frac{x_{i}}{\sigma_{i}^{2}}\right)\mathrm{E}\left(\sum_{i=1}^{n}\frac{\mathring{y}_{i}}{\sigma_{i}^{2}}\right)-\left(\sum_{i=1}^{n}\frac{1}{\sigma_{i}^{2}}\right)\mathrm{E}\left(\sum_{i=1}^{n}\frac{x_{i}\mathring{y}_{i}}{\sigma_{i}^{2}}\right)}{\left(\sum_{i=1}^{n}\frac{x_{i}}{\sigma_{i}^{2}}\right)^{2}-\left(\sum_{i=1}^{n}\frac{1}{\sigma_{i}^{2}}\right)\left(\sum_{i=1}^{n}\frac{x_{i}^{2}}{\sigma_{i}^{2}}\right)}=
		\]
		\[
		=\frac{\left(\sum_{i=1}^{n}\frac{x_{i}}{\sigma_{i}^{2}}\right)\left(\sum_{i=1}^{n}\frac{\mathrm{E}\left(\mathring{y}_{i}\right)}{\sigma_{i}^{2}}\right)-\left(\sum_{i=1}^{n}\frac{1}{\sigma_{i}^{2}}\right)\left(\sum_{i=1}^{n}\frac{x_{i}\mathrm{E}\left(\mathring{y}_{i}\right)}{\sigma_{i}^{2}}\right)}{\left(\sum_{i=1}^{n}\frac{x_{i}}{\sigma_{i}^{2}}\right)^{2}-\left(\sum_{i=1}^{n}\frac{1}{\sigma_{i}^{2}}\right)\left(\sum_{i=1}^{n}\frac{x_{i}^{2}}{\sigma_{i}^{2}}\right)}
		\]
		A continuación, combinando las suposiciones \vref{assu:LEO yi} y
		\vref{assu:LEO yi tilde}, se tiene que:
		\begin{equation}
			\mathrm{E}\left(\mathring{y}_{i}\right)=\tilde{y}_{i}=ax_{i}+b\qquad\forall i=1,\dots,n\label{eq:LEO E(yi)}
		\end{equation}
		donde los parámetros $a$ y $b$ que aparecen en la ecuación anterior
		no son sus estimadores, sino sus valores verdaderos (por eso no llevan
		circunflejo). De esta forma, se tiene:
		\[
		\mathrm{E}\left(\hat{a}\right)=\frac{\left(\sum_{i=1}^{n}\frac{x_{i}}{\sigma_{i}^{2}}\right)\left(\sum_{i=1}^{n}\frac{ax_{i}+b}{\sigma_{i}^{2}}\right)-\left(\sum_{i=1}^{n}\frac{1}{\sigma_{i}^{2}}\right)\left(\sum_{i=1}^{n}\frac{x_{i}\left(ax_{i}+b\right)}{\sigma_{i}^{2}}\right)}{\left(\sum_{i=1}^{n}\frac{x_{i}}{\sigma_{i}^{2}}\right)^{2}-\left(\sum_{i=1}^{n}\frac{1}{\sigma_{i}^{2}}\right)\left(\sum_{i=1}^{n}\frac{x_{i}^{2}}{\sigma_{i}^{2}}\right)}
		\]
		Operando, llegamos a:
		\[
		\mathrm{E}\left(\hat{a}\right)=\frac{\left(\sum_{i=1}^{n}\frac{x_{i}}{\sigma_{i}^{2}}\right)\left(a\left(\sum_{i=1}^{n}\frac{x_{i}}{\sigma_{i}^{2}}\right)+b\left(\sum_{i=1}^{n}\frac{1}{\sigma_{i}^{2}}\right)\right)-\left(\sum_{i=1}^{n}\frac{1}{\sigma_{i}^{2}}\right)\left(a\left(\sum_{i=1}^{n}\frac{x_{i}^{2}}{\sigma_{i}^{2}}\right)+b\left(\sum_{i=1}^{n}\frac{x_{i}}{\sigma_{i}^{2}}\right)\right)}{\left(\sum_{i=1}^{n}\frac{x_{i}}{\sigma_{i}^{2}}\right)^{2}-\left(\sum_{i=1}^{n}\frac{1}{\sigma_{i}^{2}}\right)\left(\sum_{i=1}^{n}\frac{x_{i}^{2}}{\sigma_{i}^{2}}\right)}=
		\]
		\[
		=\frac{a\left[\left(\sum_{i=1}^{n}\frac{x_{i}}{\sigma_{i}^{2}}\right)^{2}-\left(\sum_{i=1}^{n}\frac{1}{\sigma_{i}^{2}}\right)\left(\sum_{i=1}^{n}\frac{x_{i}^{2}}{\sigma_{i}^{2}}\right)\right]+b\left[\left(\sum_{i=1}^{n}\frac{x_{i}}{\sigma_{i}^{2}}\right)\left(\sum_{i=1}^{n}\frac{1}{\sigma_{i}^{2}}\right)-\left(\sum_{i=1}^{n}\frac{1}{\sigma_{i}^{2}}\right)\left(\sum_{i=1}^{n}\frac{x_{i}}{\sigma_{i}^{2}}\right)\right]}{\left(\sum_{i=1}^{n}\frac{x_{i}}{\sigma_{i}^{2}}\right)^{2}-\left(\sum_{i=1}^{n}\frac{1}{\sigma_{i}^{2}}\right)\left(\sum_{i=1}^{n}\frac{x_{i}^{2}}{\sigma_{i}^{2}}\right)}=
		\]
		\[
		=a\frac{\left(\sum_{i=1}^{n}\frac{x_{i}}{\sigma_{i}^{2}}\right)^{2}-\left(\sum_{i=1}^{n}\frac{1}{\sigma_{i}^{2}}\right)\left(\sum_{i=1}^{n}\frac{x_{i}^{2}}{\sigma_{i}^{2}}\right)}{\left(\sum_{i=1}^{n}\frac{x_{i}}{\sigma_{i}^{2}}\right)^{2}-\left(\sum_{i=1}^{n}\frac{1}{\sigma_{i}^{2}}\right)\left(\sum_{i=1}^{n}\frac{x_{i}^{2}}{\sigma_{i}^{2}}\right)}=a
		\]
		
		De esta forma, efectivamente $\mathrm{E}\left(\hat{a}\right)=a$ y,
		por tanto, $\hat{a}$ es un operador no sesgado. Vayamos con $\hat{b}$:
		\[
		\hat{b}=\frac{\left(\sum_{i=1}^{n}\frac{\mathring{y}_{i}}{\sigma_{i}^{2}}\right)-\left(\sum_{i=1}^{n}\frac{x_{i}}{\sigma_{i}^{2}}\right)\hat{a}}{\left(\sum_{i=1}^{n}\frac{1}{\sigma_{i}^{2}}\right)}
		\]
		De nuevo, para calcular el valor esperado de $\hat{b}$, tenemos que
		considerar que todo es constante, salvo las variables aleatorias $\left\{ \mathring{y}_{i}\right\} _{i=1}^{n}$
		y el estimador $\hat{a}$ (que también es una variable aleatoria).
		De esta manera:
		\[
		\mathrm{E}\left(\hat{b}\right)=\frac{\mathrm{E}\left[\left(\sum_{i=1}^{n}\frac{\mathring{y}_{i}}{\sigma_{i}^{2}}\right)-\left(\sum_{i=1}^{n}\frac{x_{i}}{\sigma_{i}^{2}}\right)\hat{a}\right]}{\left(\sum_{i=1}^{n}\frac{1}{\sigma_{i}^{2}}\right)}=\frac{\mathrm{E}\left(\sum_{i=1}^{n}\frac{\mathring{y}_{i}}{\sigma_{i}^{2}}\right)-\left(\sum_{i=1}^{n}\frac{x_{i}}{\sigma_{i}^{2}}\right)\mathrm{E}\left(\hat{a}\right)}{\left(\sum_{i=1}^{n}\frac{1}{\sigma_{i}^{2}}\right)}=
		\]
		\[
		=\frac{\left(\sum_{i=1}^{n}\frac{\mathrm{E}\left(\mathring{y}_{i}\right)}{\sigma_{i}^{2}}\right)-\left(\sum_{i=1}^{n}\frac{x_{i}}{\sigma_{i}^{2}}\right)\mathrm{E}\left(\hat{a}\right)}{\left(\sum_{i=1}^{n}\frac{1}{\sigma_{i}^{2}}\right)}
		\]
		Al igual que en el caso anterior, combinando las suposiciones \vref{assu:LEO yi}
		y \vref{assu:LEO yi tilde}, se tiene la ecuación \vref{eq:LEO E(yi)}.
		Además, como hemos probado antes $\mathrm{E}\left(\hat{a}\right)=a$.
		Así, se obtiene:
		\[
		\mathrm{E}\left(\hat{b}\right)=\frac{\left(\sum_{i=1}^{n}\frac{ax_{i}+b}{\sigma_{i}^{2}}\right)-\left(\sum_{i=1}^{n}\frac{x_{i}}{\sigma_{i}^{2}}\right)a}{\left(\sum_{i=1}^{n}\frac{1}{\sigma_{i}^{2}}\right)}=
		\]
		\[
		=\frac{a\left(\sum_{i=1}^{n}\frac{x_{i}}{\sigma_{i}^{2}}\right)+b\left(\sum_{i=1}^{n}\frac{1}{\sigma_{i}^{2}}\right)-\left(\sum_{i=1}^{n}\frac{x_{i}}{\sigma_{i}^{2}}\right)a}{\left(\sum_{i=1}^{n}\frac{1}{\sigma_{i}^{2}}\right)}=
		\]
		\[
		=\frac{a\left[\left(\sum_{i=1}^{n}\frac{x_{i}}{\sigma_{i}^{2}}\right)-\left(\sum_{i=1}^{n}\frac{x_{i}}{\sigma_{i}^{2}}\right)\right]+b\left(\sum_{i=1}^{n}\frac{1}{\sigma_{i}^{2}}\right)}{\left(\sum_{i=1}^{n}\frac{1}{\sigma_{i}^{2}}\right)}=b\frac{\left(\sum_{i=1}^{n}\frac{1}{\sigma_{i}^{2}}\right)}{\left(\sum_{i=1}^{n}\frac{1}{\sigma_{i}^{2}}\right)}=b
		\]
		Y, por consiguiente, $\mathrm{E}\left(\hat{b}\right)=b$ y $\hat{b}$
		es un operador no sesgado.
	\end{proof}
	\begin{prop}
		La matriz de covarianza de los estimadores $\hat{a},\hat{b}$ satisface:
		\[
		\mathrm{Cov}\left(\hat{a},\hat{b}\right)\ge\frac{{\displaystyle 1}}{{\displaystyle \left(\sum_{i=1}^{n}\frac{x_{i}^{2}}{\sigma_{i}^{2}}\right)\left(\sum_{i=1}^{n}\frac{1}{\sigma_{i}^{2}}\right)-\left(\sum_{i=1}^{n}\frac{x_{i}}{\sigma_{i}^{2}}\right)^{2}}}\left(\begin{matrix}{\displaystyle \sum_{i=1}^{n}\frac{1}{\sigma_{i}^{2}}} & {\displaystyle -\sum_{i=1}^{n}\frac{x_{i}}{\sigma_{i}^{2}}}\\
			{\displaystyle -\sum_{i=1}^{n}\frac{x_{i}}{\sigma_{i}^{2}}} & {\displaystyle \sum_{i=1}^{n}\frac{x_{i}^{2}}{\sigma_{i}^{2}}}
		\end{matrix}\right)
		\]
		donde la desigualdad $\ge$ se entiende como $A\ge B\iff A-B$ es
		semidefinida positiva.
	\end{prop}
	\begin{proof}
		En la demostración de la proposición \vref{prop:LEO m=0000E1xima verosimliitud}
		llegamos a las siguientes expresiones para las derivadas parciales
		del logaritmo de la función de verosimilitud (ecuaciones \vref{eq:LEO partial a}
		y \vref{eq:LEO partial b}):
		\[
		\frac{\partial l}{\partial a}=\sum_{i=1}^{n}x_{i}\frac{y_{i}-ax_{i}-b}{\sigma_{i}^{2}}
		\]
		\[
		\frac{\partial l}{\partial b}=\sum_{i=1}^{n}\frac{y_{i}-ax_{i}-b}{\sigma_{i}^{2}}
		\]
		A continuación, debemos hallar las derivadas segundas:
		\[
		\frac{\partial^{2}l}{\partial a^{2}}=-\sum_{i=1}^{n}\frac{x_{i}^{2}}{\sigma_{i}^{2}}\qquad\frac{\partial^{2}l}{\partial b^{2}}=-\sum_{i=1}^{n}\frac{1}{\sigma_{i}^{2}}\qquad\frac{\partial^{2}l}{\partial a\partial b}=-\sum_{i=1}^{n}\frac{x_{i}}{\sigma_{i}^{2}}
		\]
		De esta forma, el hessiano del logaritmo de la función de verosimilitud
		queda:
		\[
		\mathrm{H}l=\left(\begin{matrix}-\sum_{i=1}^{n}\frac{x_{i}^{2}}{\sigma_{i}^{2}} & -\sum_{i=1}^{n}\frac{x_{i}}{\sigma_{i}^{2}}\\
			-\sum_{i=1}^{n}\frac{x_{i}}{\sigma_{i}^{2}} & -\sum_{i=1}^{n}\frac{1}{\sigma_{i}^{2}}
		\end{matrix}\right)
		\]
		Y, por ende, la matriz de información de Fisher resulta:
		\[
		I=\mathrm{E}\left[-\mathrm{H}l\right]=\left(\begin{matrix}\sum_{i=1}^{n}\frac{x_{i}^{2}}{\sigma_{i}^{2}} & \sum_{i=1}^{n}\frac{x_{i}}{\sigma_{i}^{2}}\\
			\sum_{i=1}^{n}\frac{x_{i}}{\sigma_{i}^{2}} & \sum_{i=1}^{n}\frac{1}{\sigma_{i}^{2}}
		\end{matrix}\right)
		\]
		Sabemos que la matriz de información de Fisher satisface la desigualdad:
		\[
		\mathrm{Cov}\left(\hat{a},\hat{b}\right)\ge I^{-1}=\frac{1}{\left(\sum_{i=1}^{n}\frac{x_{i}^{2}}{\sigma_{i}^{2}}\right)\left(\sum_{i=1}^{n}\frac{1}{\sigma_{i}^{2}}\right)-\left(\sum_{i=1}^{n}\frac{x_{i}}{\sigma_{i}^{2}}\right)^{2}}\left(\begin{matrix}\sum_{i=1}^{n}\frac{1}{\sigma_{i}^{2}} & -\sum_{i=1}^{n}\frac{x_{i}}{\sigma_{i}^{2}}\\
			-\sum_{i=1}^{n}\frac{x_{i}}{\sigma_{i}^{2}} & \sum_{i=1}^{n}\frac{x_{i}^{2}}{\sigma_{i}^{2}}
		\end{matrix}\right)
		\]
		donde hemos aplicado que la inversa de una matriz 2x2 viene dada por
		la fórmula:
		\[
		\left(\begin{matrix}a & b\\
			c & d
		\end{matrix}\right)^{-1}=\frac{1}{ad-bc}\left(\begin{matrix}d & -b\\
			-c & a
		\end{matrix}\right)
		\]
	\end{proof}
	\begin{prop}
		La variable aleatoria:
		\[
		\mathring{Z}=\sum_{i=1}^{n}\frac{\left(\mathring{y}_{i}-ax_{i}-b\right)^{2}}{\sigma_{i}^{2}}
		\]
		sigue una distribución $\chi^{2}$ con $n-2$ grados de libertad.
	\end{prop}
	\begin{proof}
		Por la suposición \vref{assu:LEO yi}, $\mathring{y}_{i}\sim\mathcal{N}\left(\tilde{y}_{i},\sigma_{i}\right)\;\forall i=1,\dots,n$.
		Además, por la suposición \vref{assu:LEO yi tilde}, se tiene:
		\[
		\mathring{y}_{i}\sim\mathcal{N}\left(ax_{i}+b,\sigma_{i}\right)\qquad\forall i=1,\dots,n
		\]
		Por tanto:
		\[
		\mathring{\varepsilon}_{i}\coloneqq\frac{\mathring{y}_{i}-\left(ax_{i}+b\right)}{\sigma_{i}}=\frac{\mathring{y}_{i}-ax_{i}-b}{\sigma_{i}}\sim\mathcal{N}\left(0,1\right)\qquad\forall i=1,\dots,n
		\]
		por la proposición \vref{prop:gaussiana normalizable}. Por otra parte,
		las expresiones de $\hat{a}$ y $\hat{b}$ obtenidas en la proposición
		\vref{prop:LEO m=0000E1xima verosimliitud} imponen dos ligaduras
		en los posibles valores de las variables aleatorias $\left\{ \mathring{\varepsilon}_{i}\right\} _{i=1}^{n}$.
		Por tanto, el vector aleatorio $\mathring{\vec{\varepsilon}}\coloneqq\left(\mathring{\varepsilon}_{1},\dots,\mathring{\varepsilon}_{n}\right)$
		pertenece a un espacio vectorial de dimensión $n-2$. Por la proposición
		\vref{prop:vector gaussiano chi cuadrado}:
		\[
		\mathring{Z}=\sum_{i=1}^{n}\frac{\left(\mathring{y}_{i}-ax_{i}-b\right)^{2}}{\sigma_{i}^{2}}=\sum_{i=1}^{n}\mathring{\varepsilon}_{i}^{2}=\left|\left|\mathring{\vec{\varepsilon}}\right|\right|^{2}\sim\chi_{n-2}^{2}
		\]
	\end{proof}
	\begin{lyxalgorithm}
		Para implementar todo esto en el ordenador resulta más sencillo definirse
		las cantidades:
		\[
		S_{\sigma}\coloneqq\sum_{i=1}^{n}\frac{1}{\sigma_{i}^{2}}\qquad S_{x}\coloneqq\sum_{i=1}^{n}\frac{x_{i}}{\sigma_{i}^{2}}\qquad S_{xx}\coloneqq\sum_{i=1}^{n}\frac{x_{i}^{2}}{\sigma_{i}^{2}}\qquad S_{y}\coloneqq\sum_{i=1}^{n}\frac{y_{i}}{\sigma_{i}^{2}}\qquad S_{xy}\coloneqq\sum_{i=1}^{n}\frac{x_{i}y_{i}}{\sigma_{i}^{2}}
		\]
		De esta forma, se obtienen las siguientes expresiones:
		\[
		\hat{a}=\frac{S_{x}S_{y}-S_{\sigma}S_{xy}}{S_{x}^{2}-S_{\sigma}S_{xx}}
		\]
		\[
		\hat{b}=\frac{S_{y}-S_{x}\hat{a}}{S_{\sigma}}
		\]
		\[
		\mathrm{Cov}\left(\hat{a},\hat{b}\right)\ge\frac{1}{S_{xx}S_{\sigma}-S_{x}^{2}}\left(\begin{matrix}S_{\sigma} & -S_{x}\\
			-S_{x} & S_{xx}
		\end{matrix}\right)
		\]
		En la práctica, la desigualdad anterior se interpreta como una igualdad
		por comodidad.
	\end{lyxalgorithm}
	
	\section{Ajustes a funciones arbitrarias con errores únicamente en las ordenadas}\label{subsec:semigeneral}
	\begin{assumption}
		\label{assu:semigeneral xi exactos}Los datos medidos $\left\{ x_{i}\right\} _{i=1}^{n}$
		son exactos.
	\end{assumption}
	%
	\begin{assumption}
		\label{assu:semigeneral yi}Cada dato medido $y_{i}$ es una muestra
		de una distribución gaussiana de centro <<el valor real>> $\tilde{y}_{i}$
		desconocido y de varianza $\sigma_{i}^{2}$ conocida, que denotaremos
		$\mathring{y}_{i}$. Es decir:
		\[
		\mathring{y}_{i}\sim\mathcal{N}\left(\tilde{y}_{i},\sigma_{i}^{2}\right)\qquad\forall i=1,\dots,n
		\]
		Además, las variables aleatorias asociadas a los datos medidos $\left\{ \mathring{y}_{i}\right\} _{i=1}^{n}$
		son independientes dos a dos.
	\end{assumption}
	%
	\begin{assumption}
		\label{assu:semigeneral yi tilde}Los valores reales $\left\{ \tilde{y}_{i}\right\} _{i=1}^{n}$
		satisfacen la ecuación:
		\[
		\tilde{y}_{i}=g\left(x_{i},a_{1},\dots,a_{m}\right)\qquad\forall i=1,\dots,n
		\]
		donde $a_{1},\dots,a_{m}\in\mathbb{R}$ son constantes (los parámetros
		del modelo) y $g$ es una función $g:\Omega\subseteq\mathbb{R}\times\mathbb{R}^{m}\longrightarrow\mathbb{R}$,
		siendo $\Omega$ un conjunto abierto de $\mathbb{R}^{m+1}$.
	\end{assumption}
	\begin{prop}
		\label{prop:semigeneral m=0000E1xima verosimilitud}Los estimadores
		de máxima verosimilitud para los parámetros del modelo ($\hat{a}_{1},\dots,\hat{a}_{m}$)
		son los valores de $a_{1},\dots,a_{m}$ que resuelven el sistema de
		ecuaciones:
		\[
		\frac{\partial l}{\partial a_{j}}=\sum_{i=1}^{n}\frac{y_{i}-g\left(x_{i},a_{1},\dots,a_{m}\right)}{\sigma_{i}^{2}}\frac{\partial g}{\partial a_{j}}\left(x_{i},a_{1},\dots,a_{m}\right)=0\qquad\forall j=1,\dots,m
		\]
	\end{prop}
	\begin{proof}
		Por la suposición \vref{assu:semigeneral yi}, sabemos que los datos
		medidos $\left\{ y_{i}\right\} _{i=1}^{n}$ son muestras de variables
		aleatorias gaussianas de centro $\tilde{y}_{i}$ y desviación típica
		$\sigma_{i}$. Es decir:
		\[
		\mathring{y}_{i}\sim\mathcal{N}\left(\tilde{y}_{i},\sigma_{i}^{2}\right)\qquad\forall i=1,\dots,n
		\]
		Ahora bien, por la suposición \vref{assu:semigeneral yi tilde}, se
		tiene $\tilde{y}_{i}=g\left(x_{i},a_{1},\dots,a_{m}\right)$. Entonces,
		las variables aleatorias $\left\{ \mathring{y}_{i}\right\} _{i=1}^{n}$
		siguen distribuciones normales de centro $g\left(x_{i},a_{1},\dots,a_{m}\right)$
		y de desviación típica $\sigma_{i}$. Dicho de otra forma:
		\[
		\mathring{y}_{i}\sim\mathcal{N}\left(g\left(x_{i},a_{1},\dots,a_{m}\right),\sigma_{i}^{2}\right)\qquad\forall i=1,\dots,n
		\]
		Sabemos que lo anterior está bien definido ya que los $\left\{ x_{i}\right\} _{i=1}^{n}$
		son exactos por la suposición \vref{assu:semigeneral xi exactos}.
		
		De esta manera, la función de densidad de probabilidad de cada variable
		aleatoria $\mathring{y}_{i}$ viene dada por:
		\[
		f_{\mathring{y}_{i}}\left(\dot{y}_{i}\right)=\frac{1}{\sqrt{2\pi\sigma_{i}^{2}}}\exp\left(-\frac{\left(\dot{y}_{i}-g\left(x_{i},a_{1},\dots,a_{m}\right)\right)^{2}}{2\sigma_{i}^{2}}\right)\qquad\forall i=1,\dots,n
		\]
		donde no debe confundirse la variable aleatoria $\mathring{y}_{i}$
		con el argumento de la función de densidad de probabilidad $f_{\mathring{y}_{i}}$:
		$\dot{y}_{i}$. 
		
		A continuación, como por la suposición \vref{assu:semigeneral yi},
		las variables aleatorias $\left\{ \mathring{y}_{i}\right\} _{i=1}^{n}$
		son independientes dos a dos, la función de verosimilitud será simplemente
		el producto de las funciones de densidad $f_{\mathring{y}_{i}}$ evaluadas
		en los datos medidos:
		\[
		L\left(a,b\right)=\prod_{i=1}^{n}f_{\mathring{y}_{i}}\left(\dot{y}_{i}=y_{i}\right)=\frac{1}{\sqrt{2^{n}\pi^{n}\prod_{i=1}^{n}\sigma_{i}^{2}}}\exp\left(-\sum_{i=1}^{n}\left(\frac{y_{i}-g\left(x_{i},a_{1},\dots,a_{m}\right)}{\sqrt{2}\sigma_{i}}\right)^{2}\right)
		\]
		donde hemos usado la propiedad $\mathrm{e}^{a}\mathrm{e}^{b}=\mathrm{e}^{a+b}\;\forall a,b\in\mathbb{R}$.
		Para proseguir, consideramos el logaritmo de la función de verosimilitud:
		\[
		l\left(a,b\right)=\ln L=\ln\left(\frac{1}{\sqrt{2^{n}\pi^{n}\prod_{i=1}^{n}\sigma_{i}^{2}}}\right)-\sum_{i=1}^{n}\left(\frac{y_{i}-g\left(x_{i},a_{1},\dots,a_{m}\right)}{\sqrt{2}\sigma_{i}}\right)^{2}
		\]
		donde hemos hecho uso de la propiedad del logaritmo de transformar
		productos en sumas y hemos empleado, asimismo, que la exponencial
		y el logaritmo son operaciones inversas.
		
		Con el fin de obtener los operadores de máxima verosimilitud, debemos
		buscar aquellos puntos en los que $l$ es máxima (que ya sabemos que
		coincidirán con los de $L$ por ser el logaritmo neperiano una función
		estrictamente creciente). Para ello, derivamos la función $l$ con
		respecto a cada parámetro $a_{j}$:
		\[
		\frac{\partial l}{\partial a_{j}}=-\sum_{i=1}^{n}2\frac{y_{i}-g\left(x_{i},a_{1},\dots,a_{m}\right)}{\sqrt{2}\,\sigma_{i}}\left(-\frac{1}{\sqrt{2}\,\sigma_{i}}\right)\frac{\partial g}{\partial a_{j}}\left(x_{i},a_{1},\dots,a_{m}\right)=
		\]
		\[
		=\sum_{i=1}^{n}\frac{y_{i}-g\left(x_{i},a_{1},\dots,a_{m}\right)}{\sigma_{i}^{2}}\frac{\partial g}{\partial a_{j}}\left(x_{i},a_{1},\dots,a_{m}\right)
		\]
		Los estimadores de máxima verosimilitud $\hat{a}_{1},\dots,\hat{a}_{m}$
		son los valores de $a_{1},\dots,a_{m}$ tales que:
		\[
		\frac{\partial l}{\partial a_{j}}=0\qquad\forall j=1,\dots,m\iff
		\]
		\[
		\sum_{i=1}^{n}\frac{y_{i}-g\left(x_{i},a_{1},\dots,a_{m}\right)}{\sigma_{i}^{2}}\frac{\partial g}{\partial a_{j}}\left(x_{i},a_{1},\dots,a_{m}\right)=0\qquad\forall j=1,\dots,m
		\]
	\end{proof}
	\begin{prop}
		\label{prop:semigeneral Fisher}La matriz de información de Fisher
		viene dada por:
		\[
		I_{j,k}=\sum_{i=1}^{n}\frac{1}{\sigma_{i}^{2}}\frac{\partial g}{\partial a_{k}}\left(x_{i},a_{1},\dots,a_{m}\right)\frac{\partial g}{\partial a_{j}}\left(x_{i},a_{1},\dots,a_{m}\right)\qquad\forall j,k=1,\dots,m
		\]
	\end{prop}
	\begin{proof}
		Recuperamos la expresión para la derivada del logaritmo de la función
		de verosimilitud respecto a un parámetro $a_{j}$ dada en la proposición
		\vref{prop:semigeneral m=0000E1xima verosimilitud}:
		\[
		\frac{\partial l}{\partial a_{j}}=\sum_{i=1}^{n}\frac{y_{i}-g\left(x_{i},a_{1},\dots,a_{m}\right)}{\sigma_{i}^{2}}\frac{\partial g}{\partial a_{j}}\left(x_{i},a_{1},\dots,a_{m}\right)
		\]
		Volviendo a derivar con respecto a $a_{k}$, se tiene:
		\[
		\frac{\partial^{2}l}{\partial a_{k}\partial a_{j}}=\sum_{i=1}^{n}\left[-\frac{1}{\sigma_{i}^{2}}\frac{\partial g}{\partial a_{k}}\left(x_{i},a_{1},\dots,a_{m}\right)\frac{\partial g}{\partial a_{j}}\left(x_{i},a_{1},\dots,a_{m}\right)+\right.
		\]
		\[
		\left.+\frac{y_{i}-g\left(x_{i},a_{1},\dots,a_{m}\right)}{\sigma_{i}^{2}}\frac{\partial^{2}g}{\partial a_{k}\partial a_{j}}\left(x_{i},a_{1},\dots,a_{m}\right)\right]
		\]
		Ahora bien, sabemos que el elemento $j,k$ de la matriz de información
		de Fisher viene dado por:
		\[
		I_{j,k}=\mathrm{E}\left(-\left(\mathrm{H}l\right)_{j,k}\right)=\mathrm{E}\left(-\frac{\partial^{2}l}{\partial a_{k}\partial a_{j}}\right)
		\]
		donde en la parcial segunda correspondiente hay que sustituir $y_{i}$
		por $\mathring{y}_{i}$, su variable aleatoria correspondiente.
		\[
		I_{j,k}=\mathrm{E}\left[\sum_{i=1}^{n}\left[\frac{1}{\sigma_{i}^{2}}\frac{\partial g}{\partial a_{k}}\left(x_{i},a_{1},\dots,a_{m}\right)\frac{\partial g}{\partial a_{j}}\left(x_{i},a_{1},\dots,a_{m}\right)+\right.\right.
		\]
		\[
		\left.\left.-\frac{\mathring{y}_{i}-g\left(x_{i},a_{1},\dots,a_{m}\right)}{\sigma_{i}^{2}}\frac{\partial^{2}g}{\partial a_{k}\partial a_{j}}\left(x_{i},a_{1},\dots,a_{m}\right)\right]\right]=
		\]
		\[
		=\sum_{i=1}^{n}\frac{1}{\sigma_{i}^{2}}\frac{\partial g}{\partial a_{k}}\left(x_{i},a_{1},\dots,a_{m}\right)\frac{\partial g}{\partial a_{j}}\left(x_{i},a_{1},\dots,a_{m}\right)+
		\]
		\[
		-\sum_{i=1}^{n}\frac{\mathrm{E}\left(\mathring{y}_{i}\right)-g\left(x_{i},a_{1},\dots,a_{m}\right)}{\sigma_{i}^{2}}\frac{\partial^{2}g}{\partial a_{k}\partial a_{j}}\left(x_{i},a_{1},\dots,a_{m}\right)
		\]
		Combinando las suposiciones \vref{assu:semigeneral yi} y \vref{assu:semigeneral yi tilde},
		se tiene que:
		\[
		\mathrm{E}\left(\mathring{y}_{i}\right)=g\left(x_{i},a_{1},\dots,a_{m}\right)\qquad\forall i=1,\dots,n
		\]
		Y, por tanto, el elemento $j,k$ de la matriz de información de Fisher
		queda, simplemente:
		\[
		I_{j,k}=\sum_{i=1}^{n}\frac{1}{\sigma_{i}^{2}}\frac{\partial g}{\partial a_{k}}\left(x_{i},a_{1},\dots,a_{m}\right)\frac{\partial g}{\partial a_{j}}\left(x_{i},a_{1},\dots,a_{m}\right)
		\]
		
	\end{proof}
	\begin{prop}\label{prop:semigeneral chi2}
		La variable aleatoria:
		\[
		\mathring{Z}=\sum_{i=1}^{n}\frac{\left(\mathring{y}_{i}-g\left(x_{i},a_{1},\dots,a_{m}\right)\right)^{2}}{\sigma_{i}^{2}}
		\]
		sigue una distribución $\chi^{2}$ con $n-m$ grados de libertad.
	\end{prop}
	\begin{proof}
		Por la suposición \vref{assu:semigeneral yi}, $\mathring{y}_{i}\sim\mathcal{N}\left(\tilde{y}_{i},\sigma_{i}^{2}\right)\;\forall i=1,\dots,n$.
		Además, por la suposición \vref{assu:semigeneral yi tilde}, se tiene:
		\[
		\mathring{y}_{i}\sim\mathcal{N}\left(g\left(x_{i},a_{1},\dots,a_{m}\right),\sigma_{i}^{2}\right)\qquad\forall i=1,\dots,n
		\]
		Por tanto:
		\[
		\mathring{\varepsilon}_{i}\coloneqq\frac{\mathring{y}_{i}-g\left(x_{i},a_{1},\dots,a_{m}\right)}{\sigma_{i}}\sim\mathcal{N}\left(0,1\right)\qquad\forall i=1,\dots,n
		\]
		por la proposición \vref{prop:gaussiana normalizable}. Por otra parte,
		las ecuaciones obtenidas en la proposición \vref{prop:semigeneral m=0000E1xima verosimilitud}
		imponen $m$ ligaduras en los posibles valores de las variables aleatorias
		$\left\{ \mathring{\varepsilon}_{i}\right\} _{i=1}^{n}$. Por tanto,
		el vector aleatorio $\mathring{\vec{\varepsilon}}\coloneqq\left(\mathring{\varepsilon}_{1},\dots,\mathring{\varepsilon}_{n}\right)$
		pertenece a un espacio vectorial de dimensión $n-m$. Por la proposición
		\vref{prop:vector gaussiano chi cuadrado}:
		\[
		\mathring{Z}=\sum_{i=1}^{n}\frac{\left(\mathring{y}_{i}-g\left(x_{i},a_{1},\dots,a_{m}\right)\right)^{2}}{\sigma_{i}^{2}}=\sum_{i=1}^{n}\mathring{\varepsilon}_{i}^{2}=\left|\left|\mathring{\vec{\varepsilon}}\right|\right|^{2}\sim\chi_{n-m}^{2}
		\]
	\end{proof}
	\begin{lyxalgorithm}
		Este caso resulta más complicado que los ajustes lineales puesto que
		no hay solución analítica. El método a seguir es el siguiente:
		\begin{enumerate}
			\item Se calculan analíticamente (si se puede) las derivadas de la función
			$g$: $\frac{\partial g}{\partial x_{j}}\;\forall j=1,\dots,m$.
			\item Se resuelven numéricamente las ecuaciones \vref{prop:semigeneral m=0000E1xima verosimilitud}.
			\[
			\frac{\partial l}{\partial a_{j}}=\sum_{i=1}^{n}\frac{y_{i}-g\left(x_{i},a_{1},\dots,a_{m}\right)}{\sigma_{i}^{2}}\frac{\partial g}{\partial a_{j}}\left(x_{i},a_{1},\dots,a_{m}\right)=0\qquad\forall j=1,\dots,m
			\]
			\item Se calcula la matriz de información de Fisher mediante la expresión
			dada en la proposición \vref{prop:semigeneral Fisher}:
			\[
			I_{j,k}=\sum_{i=1}^{n}\frac{1}{\sigma_{i}^{2}}\frac{\partial g}{\partial a_{k}}\left(x_{i},a_{1},\dots,a_{m}\right)\frac{\partial g}{\partial a_{j}}\left(x_{i},a_{1},\dots,a_{m}\right)\qquad\forall j,k=1,\dots,m
			\]
			\item Se invierte la matriz de información de Fisher numéricamente para
			obtener la matriz de covarianza:
			\[
			\mathrm{Cov}\left(\hat{a}_{1},\dots,\hat{a}_{m}\right)=I^{-1}
			\]
		\end{enumerate}
	\end{lyxalgorithm}
	
\end{document}


